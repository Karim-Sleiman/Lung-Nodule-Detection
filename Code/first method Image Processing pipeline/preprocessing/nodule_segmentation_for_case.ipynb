{"cells":[{"cell_type":"markdown","metadata":{"id":"hhJZ2xVMOtf3"},"source":["# CONNECT TO DRIVE AND IMPORTS"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31297,"status":"ok","timestamp":1717180358113,"user":{"displayName":"Áron B. Gimesi","userId":"03233526977624235264"},"user_tz":-120},"id":"SbRlfrRqOphP","outputId":"f880e770-c946-4f6d-87fd-aa9d212a0007"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting SimpleITK\n","  Downloading SimpleITK-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: SimpleITK\n","Successfully installed SimpleITK-2.3.1\n","Mounted at /content/drive\n"]}],"source":["!pip install SimpleITK\n","\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yz2zWEPAOqAu"},"outputs":[],"source":["import os\n","import sys\n","import inspect\n","import numpy as np\n","import pandas as pd\n","import cv2 as cv\n","import matplotlib.pyplot as plt\n","\n","import SimpleITK as sitk\n","\n","from google.colab.patches import cv2_imshow"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2742,"status":"ok","timestamp":1717180361736,"user":{"displayName":"Áron B. Gimesi","userId":"03233526977624235264"},"user_tz":-120},"id":"5ebsc6oKOsdn","outputId":"c8410bee-9c4b-4583-a16a-4585c3b52e80"},"outputs":[{"name":"stdout","output_type":"stream","text":["['annotations_by_uid', 'binarize_lung', 'binarize_lung_3d', 'binary_closing', 'binary_dilation', 'binary_erosion', 'binary_fill_holes', 'binary_opening', 'center_of_mass', 'clear_border', 'create_3d_mask', 'create_annotations_mask', 'create_annotations_mask_by_uid', 'create_patch', 'debugger', 'draw_ellipsoid', 'find_by_uid', 'get_slice_candidates', 'get_slices', 'get_uids', 'img_by_uid', 'masked_annotations_by_uid', 'masked_annotations_with_info_by_uid', 'meta_by_uid', 'norm2float', 'norm2uint16', 'norm2uint8', 'normalize_intensity', 'plot_slices', 'process_slice_candidates', 'process_slices', 'remove_non_central_objects', 'segment_lung', 'segment_lung_3d', 'show_3_images', 'subset_by_uid', 'unwanted_object_filter']\n"]}],"source":["path = \"/content/drive/Shareddrives/IA DL_project/ML IA/IMAGE_PROCESSING_PIPELINE\"\n","\n","if path not in sys.path:\n","  sys.path.append(path)\n","\n","import luna_module\n","from luna_module import *\n","\n","# List all function names in the luna_module\n","function_names = [name for name, obj in inspect.getmembers(luna_module) if inspect.isfunction(obj)]\n","print(function_names)"]},{"cell_type":"markdown","metadata":{"id":"zSJQqqvR9ttU"},"source":["# CONSTANTS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8-bI0_bF9wKY"},"outputs":[],"source":["LUNA_PATH = os.path.join(os.getcwd(), \"drive\", \"Shareddrives\", \"IA DL_project\", \"ML IA\", \"LUNA16\")\n","\n","SUBSET = \"subset1\"\n","\n","SUBSETS_PATH = os.path.join(LUNA_PATH, \"subsets\") # path for subsets folder\n","SUBSET_PATH =  os.path.join(SUBSETS_PATH, SUBSET) # path for subsets folder\n","SAVE_FOLDER_PATH = os.path.join(LUNA_PATH, \"feature_extractions\", \"lungmaskedbefore_clipped\")\n","\n","\n","ANNOTATIONS_DF = pd.read_csv(os.path.join(LUNA_PATH, f\"{SUBSET}_annotations_expanded.csv\"), index_col=\"Unnamed: 0\")\n","\n","SUBSETS = os.listdir(SUBSET_PATH) # subset folders present\n","FILENAMES = os.listdir(SUBSET_PATH)\n","UIDS = set(map(lambda filename: os.path.splitext(filename)[0], FILENAMES))"]},{"cell_type":"markdown","metadata":{"id":"V5VgEWgeICWD"},"source":["# EXTRACT FEATURES"]},{"cell_type":"markdown","metadata":{"id":"jKATcIUZkovX"},"source":["## HAAR FEATURES"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kFPm9O6gJ6nP"},"outputs":[],"source":["import numpy as np\n","import cv2 as cv\n","import pandas as pd\n","\n","def create_center_surround_kernel(size, inner_size):\n","    kernel = np.zeros((size, size), dtype=np.float32)\n","\n","    # Define the start and end points of the inner region\n","    start = (size - inner_size) // 2\n","    end = start + inner_size\n","\n","    # Set the central region to +1\n","    kernel[start:end, start:end] = 1\n","\n","    # Set the surround region to -1\n","    kernel[:start, :] = -1\n","    kernel[end:, :] = -1\n","    kernel[:, :start] = -1\n","    kernel[:, end:] = -1\n","\n","    return kernel\n","\n","def create_four_rectangle_kernel(size):\n","    kernel = np.zeros((size, size), dtype=np.float32)\n","\n","    half_size = size // 2\n","\n","    # Top-left and bottom-right rectangles (+1)\n","    kernel[:half_size, :half_size] = 1\n","    kernel[half_size:, half_size:] = 1\n","\n","    # Top-right and bottom-left rectangles (-1)\n","    kernel[:half_size, half_size:] = -1\n","    kernel[half_size:, :half_size] = -1\n","\n","    return kernel\n","\n","def extract_haar_features(p_haar, kernel):\n","    # Ensure the patch and kernel are float32\n","    p_haar = p_haar.astype(np.float32)\n","    kernel = kernel.astype(np.float32)\n","\n","    # Apply the kernel to the patch using convolution\n","    filtered_patch = cv.filter2D(p_haar, -1, kernel)\n","    return filtered_patch\n","\n","def multi_scale_haar_features(p_haar, kernel, scales):\n","    haar_features_list = []\n","\n","    for scale in scales:\n","        # Resize the patch\n","        p_haar = p_haar.astype('float32')\n","\n","        row_th = int(p_haar.shape[0]*scale)\n","        col_th = int(p_haar.shape[1]*scale)\n","\n","        scaled_patch = cv.resize(p_haar, (row_th, col_th))\n","        # scaled_patch = cv.resize(p_haar, (row_th, col_th), fx=scale, fy=scale, interpolation=cv.INTER_LINEAR)\n","\n","        # Ensure the scaled patch is valid\n","        if scaled_patch.size == 0:\n","            raise ValueError(f\"Scaled patch is empty at scale {scale}\")\n","\n","        # Extract Haar features from the scaled patch\n","        haar_features = extract_haar_features(scaled_patch, kernel)\n","        haar_features_list.append(haar_features)\n","\n","        # Find the location of the maximum response\n","        min_val, max_val, min_loc, max_loc = cv.minMaxLoc(haar_features)\n","\n","    return haar_features_list\n","\n","\n","# Function to pad a feature map to a specified size\n","def pad_feature_map(feature_map, target_size):\n","    padded_feature_map = np.zeros((target_size, target_size), dtype=np.float32)\n","    current_size = feature_map.shape[0]\n","\n","    # Calculate padding offsets\n","    offset = (target_size - current_size) // 2\n","\n","    # Place the feature map in the center of the padded array\n","    padded_feature_map[offset:offset + current_size, offset:offset + current_size] = feature_map\n","\n","    return padded_feature_map\n","\n","def combine_and_average_feature_maps(feature_maps, target_size):\n","    padded_feature_maps = [pad_feature_map(fm, target_size) for fm in feature_maps]\n","    combined_feature_map = np.mean(padded_feature_maps, axis=0)\n","\n","    return combined_feature_map\n","\n","def combine_and_weighted_average_feature_maps(feature_maps, target_size, weights):\n","    weighted_sum = np.zeros((target_size, target_size), dtype=np.float32)\n","    total_weight = 0\n","\n","    for feature_map, weight in zip(feature_maps, weights):\n","        padded_feature_map = pad_feature_map(feature_map, target_size)\n","        weighted_sum += padded_feature_map * weight\n","        total_weight += weight\n","\n","    combined_feature_map = weighted_sum / total_weight\n","    return combined_feature_map\n","\n","def custom_pooling(feature_map, kernel_size, pooling_type='avg'):\n","    h, w = feature_map.shape\n","    pooled_map = np.zeros((h // kernel_size, w // kernel_size), dtype=np.float32)\n","\n","    for i in range(0, h, kernel_size):\n","        for j in range(0, w, kernel_size):\n","            patch = feature_map[i:i + kernel_size, j:j + kernel_size]\n","            if pooling_type == 'avg':\n","                pooled_map[i // kernel_size, j // kernel_size] = np.mean(patch)\n","            elif pooling_type == 'max':\n","                pooled_map[i // kernel_size, j // kernel_size] = np.max(patch)\n","            else:\n","                raise ValueError(\"Pooling type must be 'avg' or 'max'\")\n","\n","    return pooled_map\n","\n","def haar_pooled(patch, kerneltype):\n","    \"\"\"\n","    Perform multi-scale Haar feature extraction and pooling on the input patch.\n","\n","    Args:\n","        p_haar (np.ndarray): The input patch.\n","        kerneltype (str): Type of Haar kernel ('cs' for center-surround, 'fr' for four-rectangle).\n","\n","    Returns:\n","        tuple: Pooled feature maps (average pooling, max pooling).\n","    \"\"\"\n","    # Initialize Haar parameters\n","    kernel_size_cs = 8\n","    inner_size_cs = 4\n","    size_fr = 8\n","\n","    # Define scales based on patch size\n","    scales = [1, 0.875, 0.75, 0.625, 0.5, 0.375, 0.25, 0.125] if patch.shape[0] == 64 else [1, 0.875, 0.75, 0.625, 0.5, 0.375, 0.25]\n","\n","    # Calculate weights for the scales\n","    weights = [1 / scale for scale in scales]\n","    weights = [w / sum(weights) for w in weights]\n","\n","    # Target size of the patch\n","    target_size = 64\n","\n","    # Pooling parameters\n","    kernel_poolsize = 8\n","\n","    # Select the appropriate kernel\n","    if kerneltype == 'cs':\n","        kernel = create_center_surround_kernel(kernel_size_cs, inner_size_cs)\n","    elif kerneltype == 'fr':\n","        kernel = create_four_rectangle_kernel(size_fr)\n","    else:\n","        raise ValueError(\"Kernel type must be 'cs' (center-surround) or 'fr' (four-rectangle)\")\n","\n","    # Extract Haar features at multiple scales\n","    haar_features_list = multi_scale_haar_features(patch, kernel, scales)\n","\n","    # Combine and weight the feature maps\n","    combined_feature_map = combine_and_weighted_average_feature_maps(haar_features_list, target_size, weights)\n","    # combined_feature_map = combine_and_average_feature_maps(haar_features_list, target_size)\n","\n","    # Perform custom pooling\n","    pooled_feature_map_avg = custom_pooling(combined_feature_map, kernel_poolsize, pooling_type='avg')\n","    pooled_feature_map_max = custom_pooling(combined_feature_map, kernel_poolsize, pooling_type='max')\n","\n","    return pooled_feature_map_avg, pooled_feature_map_max\n","\n","\n","# Example usage\n","# p_haar = #your patch\n","# pooled_avg, pooled_max = haar_pooled(p_haar, 'cs')\n"]},{"cell_type":"markdown","metadata":{"id":"9wJubs0Zks6I"},"source":["## FEATURE EXTRACT FUNCTION"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"heMRldzAKLVK"},"outputs":[],"source":["from skimage.feature import graycomatrix, graycoprops, local_binary_pattern\n","from skimage.measure import regionprops, label\n","from skimage import io, color\n","\n","def debugger(img, title=None):\n","    if title is not None:\n","        print(title)\n","    print(np.unique(img))\n","    plt.imshow(img, cmap=\"gray\")\n","    plt.show()\n","\n","\n","def mask_closest_object_to_center(binary_image, debug=False):\n","    if debug:\n","      debugger(binary_image, \"mask_closest_object_to_center 1st step\")\n","\n","    # Label connected components\n","    labeled_image = label(binary_image)\n","\n","    if debug:\n","      debugger(labeled_image, \"mask_closest_object_to_center 2nd step\")\n","\n","    # Get image center\n","    image_center = np.array(binary_image.shape) / 2\n","\n","\n","    region_properties = regionprops(labeled_image)\n","    if not len(region_properties):\n","      raise Exception(\"No labels were found\")\n","\n","    # Initialize variables to track the closest object\n","    min_distance = float('inf')\n","    closest_label = None\n","\n","    # Iterate through each labeled object\n","    for region in region_properties:\n","        # Compute the centroid of the object\n","        centroid = region.centroid\n","\n","        # Calculate the Euclidean distance to the image center\n","        distance = np.linalg.norm(np.array(centroid) - image_center)\n","\n","        # Update the closest object if this one is closer\n","        if distance < min_distance:\n","            min_distance = distance\n","            closest_label = region.label\n","\n","    # Create a mask for the closest object\n","    closest_mask = np.zeros_like(binary_image, dtype=bool)\n","    closest_mask[labeled_image == closest_label] = 1\n","    if debug:\n","      debugger(closest_mask, \"mask_closest_object_to_center 3rd step\")\n","\n","    closest_mask = binary_fill_holes(closest_mask)\n","\n","    # Dilation to keep everything that might have been clipped out of the candidate\n","    closest_mask = binary_dilation(closest_mask, morphology.disk(1))\n","    if debug:\n","      debugger(closest_mask, \"mask_closest_object_to_center 4th step\")\n","\n","    return closest_mask.astype(float)\n","\n","from skimage.feature import hog\n","from skimage import exposure\n","\n","def extract_hog_features(patch):\n","    \"\"\"\n","    Extract Histogram of Oriented Gradients (HOG) features from a given patch.\n","\n","    Args:\n","        patch (np.ndarray): The input image patch (64x64).\n","\n","    Returns:\n","        tuple: HOG features and HOG image.\n","    \"\"\"\n","    # Ensure the patch is of type float32\n","    patch = patch.astype(np.float32)\n","\n","    # Compute HOG features\n","    hog_features = hog(patch,\n","                        orientations=9,\n","                        pixels_per_cell=(8, 8),\n","                        cells_per_block=(2, 2),\n","                        visualize=False)\n","\n","    return hog_features\n","\n","def extract_features(nodule_roi, verbose=False, debug=False):\n","\n","  if debug:\n","    debugger(nodule_roi, \"nodule_roi\")\n","\n","  n_th = -200\n","  p_th = 200\n","\n","  nodule_roi[nodule_roi<n_th] = AIR_TH\n","  nodule_roi[nodule_roi>p_th] = AIR_TH\n","\n","  if debug:\n","    debugger(nodule_roi, \"nodule_roi_clipped\")\n","\n","  if len(nodule_roi[nodule_roi!=AIR_TH].nonzero()[0]) < 3:\n","    raise Exception(\"After clipping no information was left\")\n","\n","  ret, otsu_img = cv.threshold(norm2uint8(nodule_roi), 0, 255, cv.THRESH_BINARY + cv.THRESH_OTSU)\n","  ret, binary_th_img = cv.threshold(norm2uint8(nodule_roi), 127, 255, cv.THRESH_BINARY)\n","\n","  if debug:\n","    debugger(otsu_img, \"otsu_img\")\n","  if debug:\n","    debugger(binary_th_img, \"binary_th_img\")\n","\n","\n","  center_object_mask = mask_closest_object_to_center(otsu_img, debug)\n","  if debug:\n","    debugger(center_object_mask, \"center_object_mask\")\n","\n","  labeled_nodule = label(center_object_mask)\n","  if debug:\n","    debugger(labeled_nodule, \"labeled_nodule\")\n","\n","  props = regionprops(labeled_nodule)[0]\n","  if debug:\n","    print(props)\n","\n","  # Binary Shape features\n","  area = props.area\n","  if debug:\n","    print(f\"area: {area}\")\n","\n","  perimeter = props.perimeter\n","  if debug:\n","    print(f\"perimeter: {perimeter}\")\n","\n","  compactness = (perimeter ** 2) / area\n","  if debug:\n","    print(f\"compactness: {compactness}\")\n","\n","  eccentricity = props.eccentricity\n","  if debug:\n","    print(f\"eccentricity: {eccentricity}\")\n","\n","  major_axis_length = props.major_axis_length\n","  if debug:\n","    print(f\"major_axis_length: {major_axis_length}\")\n","\n","  minor_axis_length = props.minor_axis_length\n","  if debug:\n","    print(f\"minor_axis_length: {minor_axis_length}\")\n","\n","  solidity = props.solidity\n","  if debug:\n","    print(f\"solidity: {solidity}\")\n","\n","  extent = props.extent\n","  if debug:\n","    print(f\"extent: {extent}\")\n","\n","  # Texture features using GLCM\n","  nodule_roi_uint8 = norm2uint8(nodule_roi)\n","  glcm = graycomatrix(nodule_roi_uint8, distances=[1], angles=[0], levels=256, symmetric=True, normed=True)\n","\n","  contrast = graycoprops(glcm, 'contrast')[0, 0]\n","  if debug:\n","    print(f\"contrast: {contrast}\")\n","\n","  correlation = graycoprops(glcm, 'correlation')[0, 0]\n","  if debug:\n","    print(f\"correlation: {correlation}\")\n","\n","  energy = graycoprops(glcm, 'energy')[0, 0]\n","  if debug:\n","    print(f\"energy: {energy}\")\n","\n","  homogeneity = graycoprops(glcm, 'homogeneity')[0, 0]\n","  if debug:\n","    print(f\"homogeneity: {homogeneity}\")\n","\n","  # Intensity features\n","  mean_intensity = np.mean(nodule_roi)\n","  if debug:\n","    print(f\"mean_intensity: {mean_intensity}\")\n","\n","  std_intensity = np.std(nodule_roi)\n","  if debug:\n","    print(f\"std_intensity: {std_intensity}\")\n","\n","  skewness = np.mean(((nodule_roi - mean_intensity) / std_intensity) ** 3)\n","  if debug:\n","    print(f\"skewness: {skewness}\")\n","\n","  kurtosis = np.mean(((nodule_roi - mean_intensity) / std_intensity) ** 4) - 3\n","  if debug:\n","    print(f\"kurtosis: {kurtosis}\")\n","\n","  # LBP texture feature\n","  lbp = local_binary_pattern(nodule_roi, P=8, R=1, method='uniform')\n","  if debug:\n","    print(f\"lbp: {lbp}\")\n","\n","  lbp_hist, _ = np.histogram(lbp, bins=np.arange(0, 10), range=(0, 9))\n","  lbp_hist = lbp_hist / np.sum(lbp_hist)\n","  if debug:\n","    print(f\"lbp_hist: {lbp_hist}\")\n","\n","  # Haar features\n","  pooled_avg, pooled_max = haar_pooled(nodule_roi, 'cs')\n","  if debug:\n","    print(f\"pooled_avg: {pooled_avg}\")\n","    print(f\"pooled_max: {pooled_max}\")\n","  # hog = extract_hog_features(nodule_roi)\n","\n","\n","  # Aggregate features into a dictionary or array\n","  feature_dict = {\n","      'area': area,\n","      'perimeter': perimeter,\n","      'compactness': compactness,\n","      'eccentricity': eccentricity,\n","      'major_axis_length': major_axis_length,\n","      'minor_axis_length': minor_axis_length,\n","      'solidity': solidity,\n","      'extent': extent,\n","      'contrast': contrast,\n","      'correlation': correlation,\n","      'energy': energy,\n","      'homogeneity': homogeneity,\n","      'mean_intensity': mean_intensity,\n","      'std_intensity': std_intensity,\n","      'skewness': skewness,\n","      'kurtosis': kurtosis,\n","      'lbp_hist': lbp_hist.tolist(),\n","      \"haar_avg\": pooled_avg.tolist(),\n","      \"haar_max\": pooled_max.tolist()\n","      #'hog': hog\n","  }\n","\n","\n","  # Unraveling/unpacking keys with lists in them\n","  expanded_features = {}\n","\n","  # Iterate over the original dictionary\n","  for key, value in feature_dict.items():\n","      if isinstance(value, list) and all(isinstance(i, list) for i in value):\n","          # If the value is a list of lists, unpack the sublists into new keys\n","          for i, sublist in enumerate(value, start=1):\n","              for j, v in enumerate(sublist, start=1):\n","                  new_key = f\"{key}{i}{j}\"\n","                  expanded_features[new_key] = v\n","      elif isinstance(value, list):\n","          # If the value is a list, unpack the list into new keys\n","          for i, v in enumerate(value, start=1):\n","              new_key = f\"{key}{i}\"\n","              expanded_features[new_key] = v\n","      else:\n","          # If the value is not a list of lists, keep the original key-value pair\n","          expanded_features[key] = value\n","\n","  return expanded_features"]},{"cell_type":"markdown","metadata":{"id":"7a4S_LETIB1d"},"source":["## VALIDATION"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"01MWoDLFIE7V"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","def convert_annotation_df(annotations_df, verbose=False):\n","    node_coords = annotations_df[\"cartesian_coords(zyx)\"].apply(\n","        lambda x: np.array(x.replace(\"(\", \"\").replace(\")\", \"\").split(\",\")).astype(int)[::-1]\n","    )\n","    node_coords = np.array(node_coords.to_list()).reshape((len(node_coords.to_list()), 3))\n","\n","    # Create a new DataFrame to store the extracted coordinates\n","    node_coords_df = pd.DataFrame(columns=[\"x\", \"y\", \"z\"], index=annotations_df.index)\n","    node_coords_df[\"x\"] = node_coords[:, 0]\n","    node_coords_df[\"y\"] = node_coords[:, 1]\n","    node_coords_df[\"z\"] = node_coords[:, 2]\n","\n","    if verbose:\n","      print(f\"[ANNOTATIONS] - simplified\")\n","      print(node_coords_df)\n","\n","    return node_coords_df\n","\n","def find_neighborhood_indices(large_df, small_df, threshold=15, verbose=False):\n","    \"\"\"\n","    Find indices in the larger DataFrame where the coordinates are within a\n","    specified neighborhood distance from any row coordinates in the smaller DataFrame.\n","\n","    Parameters:\n","    large_df (pd.DataFrame): The larger DataFrame with 'x', 'y', 'z' columns.\n","    small_df (pd.DataFrame): The smaller DataFrame with 'x', 'y', 'z' columns.\n","    threshold (float): The neighborhood distance threshold. Default is 15.\n","\n","    Returns:\n","    np.ndarray: An array of indices from the larger DataFrame that are within the neighborhood.\n","    \"\"\"\n","    # Initialize an empty list to store the indices\n","    neighborhood_indices = []\n","    neighborhood_indices_dict= {}\n","\n","    if not len(small_df):\n","      neighborhood_indices, neighborhood_indices_dict\n","\n","    if verbose:\n","        print(f\"[FIND HITS] - Annotations to find:\")\n","        print(small_df)\n","\n","    # Iterate through each row in the smaller DataFrame\n","    for i, row in small_df.iterrows():\n","        if verbose:\n","          print(f\"Annotation with index {i}:\")\n","          print(row)\n","\n","        neighborhood_indices_dict[i] = []\n","        # Create conditions to check for vicinity in all three axes\n","        a_x = large_df[\"x\"] >= (row[\"x\"] - threshold)\n","        b_x = large_df[\"x\"] <= (row[\"x\"] + threshold)\n","        a_y = large_df[\"y\"] >= (row[\"y\"] - threshold)\n","        b_y = large_df[\"y\"] <= (row[\"y\"] + threshold)\n","        a_z = large_df[\"z\"] >= (row[\"z\"] - threshold)\n","        b_z = large_df[\"z\"] <= (row[\"z\"] + threshold)\n","\n","        # Combine conditions for all three axes\n","        vicinity_condition = a_x & b_x & a_y & b_y & a_z & b_z\n","\n","        # Find indices where all conditions are satisfied\n","        close_indices = large_df.index[vicinity_condition].tolist()\n","\n","        # Append these indices to the neighborhood_indices list\n","        neighborhood_indices.extend(close_indices)\n","        neighborhood_indices_dict[i] = (close_indices)\n","\n","    # Return unique indices as a numpy array\n","    return np.unique(neighborhood_indices), neighborhood_indices_dict\n","\n","\n","def sensitivity_score(candidates_df, annotations_df, threshold=15, verbose=False):\n","    \"\"\"\n","    Calculate the sensitivity score based on candidate detections and annotated coordinates.\n","\n","    Parameters:\n","    candidates_df (pd.DataFrame): DataFrame containing candidate coordinates with 'x', 'y', 'z' columns.\n","    annotations_df (pd.DataFrame): DataFrame containing annotated coordinates in the format with 'x', 'y', 'z' columns.\n","\n","    Returns:\n","    float: Sensitivity score, calculated as the ratio of correctly identified annotations to the total number of annotations.\n","    \"\"\"\n","    if not len(annotations_df):\n","      if verbose:\n","        print(f\"[SENSITIVITY] - No annotations for case -> no sensitivity needed\")\n","      return 0\n","\n","    # Find neighborhood indices within a threshold distance\n","    candidate_indices, candidate_dict = find_neighborhood_indices(candidates_df, annotations_df, threshold)\n","\n","    # Initialize a score counter\n","    score = 0\n","\n","    # Iterate over the candidate dictionary to calculate the score\n","    for key, val in candidate_dict.items():\n","        if len(candidate_dict[key]): # If there is a candidate in the vicinity for the annotation, it counts as a 'hit'\n","            score += 1\n","\n","    # Return the sensitivity score\n","    return score / len(annotations_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"elapsed":47,"status":"ok","timestamp":1717180364177,"user":{"displayName":"Áron B. Gimesi","userId":"03233526977624235264"},"user_tz":-120},"id":"Ryb91CPTqZrZ","outputId":"523ff533-f29a-42df-8d13-3254f113e638"},"outputs":[{"name":"stdout","output_type":"stream","text":["[ANNOTATIONS] - simplified\n","Empty DataFrame\n","Columns: [x, y, z]\n","Index: []\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"convert_annotation_df(annotations_by_uid(\\\"1\",\n  \"rows\": 0,\n  \"fields\": [\n    {\n      \"column\": \"x\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"y\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"z\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe"},"text/html":["\n","  <div id=\"df-cffe7e86-000f-48c1-97da-084cadcd91f7\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>x</th>\n","      <th>y</th>\n","      <th>z</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cffe7e86-000f-48c1-97da-084cadcd91f7')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-cffe7e86-000f-48c1-97da-084cadcd91f7 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-cffe7e86-000f-48c1-97da-084cadcd91f7');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"text/plain":["Empty DataFrame\n","Columns: [x, y, z]\n","Index: []"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["convert_annotation_df(annotations_by_uid(\"1.3.6.1.4.1.14519.5.2.1.6279.6001.187451715205085403623595258748\", ANNOTATIONS_DF), verbose=True)"]},{"cell_type":"markdown","metadata":{"id":"1Ib48XdBIPzf"},"source":["# RUN FOR ONE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VjwbUORy5mz8"},"outputs":[],"source":["ONE_EXAMPLE_TEST = False"]},{"cell_type":"markdown","metadata":{"id":"scK1AV4921o9"},"source":["## LOAD EXAMPLE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DZ807lA_24N6"},"outputs":[],"source":["uid = \"1.3.6.1.4.1.14519.5.2.1.6279.6001.187451715205085403623595258748\" # analyzed case uid"]},{"cell_type":"markdown","metadata":{"id":"cWacjy4sO3uZ"},"source":["## CANDIDATES FOR UID"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1SxYGjDWOZuE"},"outputs":[],"source":["def get_candidates_for_uid(uid, subsets_path, img_3d=None, annotations_df=None, verbose=False, debug=False):\n","  subset = None\n","  for subset_dir in os.listdir(subsets_path):\n","    filenames = os.listdir(os.path.join(subsets_path, subset_dir)) # filenames in subset directory\n","    filenames = list(map(lambda x: os.path.splitext(x)[0], filenames)) # filename w/o extension\n","    filenames = set(filenames) # unique filenames (because .mhd and .raw have same filename)\n","    if uid in filenames:\n","      subset = subset_dir\n","      break # if found -> stop\n","\n","  if subset is None:\n","    raise FileNotFoundError(\"No image under this uid\")\n","\n","  if img_3d is None:\n","    img_path = os.path.join(subsets_path, subset, f\"{uid}.mhd\") # .mhd path\n","    mhd_img = sitk.ReadImage(img_path) # SimpleITK object\n","    img_3d = sitk.GetArrayFromImage(mhd_img)\n","    img_3d[img_3d < AIR_TH] = AIR_TH\n","\n","  candidates_df, candidate_masks = process_slice_candidates(img_3d, verbose=verbose, debug=debug)\n","\n","  return candidates_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6W2I9aYJoD3H"},"outputs":[],"source":["if ONE_EXAMPLE_TEST:\n","  candidates_df = get_candidates_for_uid(uid, SUBSETS_PATH, annotations_df=ANNOTATIONS_DF, verbose=True)"]},{"cell_type":"markdown","metadata":{"id":"PYPEwT69Yz-x"},"source":["## EXTRACT FEATURES FOR UID"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9QDefdE0WSm8"},"outputs":[],"source":["def extract_features_for_uid(uid, subsets_path, candidates_df, img_3d=None, verbose=False, debug=False):\n","  subset = None\n","  for subset_dir in os.listdir(subsets_path):\n","    filenames = os.listdir(os.path.join(subsets_path, subset_dir)) # filenames in subset directory\n","    filenames = list(map(lambda x: os.path.splitext(x)[0], filenames)) # filename w/o extension\n","    filenames = set(filenames) # unique filenames (because .mhd and .raw have same filename)\n","    if uid in filenames:\n","      subset = subset_dir\n","      break # if found -> stop\n","\n","  if subset is None:\n","    raise FileNotFoundError(\"No image under this uid\")\n","\n","  if img_3d is None:\n","    img_path = os.path.join(subsets_path, subset, f\"{uid}.mhd\") # .mhd path\n","    mhd_img = sitk.ReadImage(img_path) # SimpleITK object\n","\n","    img_3d = sitk.GetArrayFromImage(mhd_img)\n","    img_3d[img_3d < AIR_TH] = AIR_TH\n","\n","  mask_3d = binarize_lung_3d(img_3d).astype(bool)\n","\n","\n","  # EXTRACT FEATURES\n","  candidate_features_df = pd.DataFrame()\n","\n","  for i, row in candidates_df.iterrows():\n","    if verbose:\n","      print(f\"[FEATURES] - extracting features for candidate #{i}\")\n","\n","    slice_ = img_3d[row[\"z\"],:,:] # get slice for candidate\n","\n","    mask = mask_3d[row[\"z\"],:,:] # get lung mask for slice\n","\n","    slice_masked = slice_.copy()\n","    slice_masked[mask==False] = AIR_TH # mask out non-lung area\n","    if debug:\n","      debugger(slice_masked, \"slice_masked\")\n","\n","    candidate_roi, _ = create_patch(slice_masked, (row[\"y\"],row[\"x\"]))\n","    if debug:\n","      debugger(candidate_roi, \"candidate_roi\")\n","\n","    try:\n","      fts = extract_features(candidate_roi, debug=debug)\n","      if verbose:\n","        print(f\"Features:\")\n","        print(fts)\n","      features_df = pd.DataFrame([fts], index=[i])\n","      candidate_features_df = pd.concat([candidate_features_df, features_df])\n","    except Exception as e: # If feature extraction fails (likely empty image) remove the candidate\n","      if verbose:\n","        print(f\"[ERROR] - {e}\")\n","        print(f\"[INFO] - dropping candidate #{i}\")\n","      candidates_df = candidates_df.drop(index=i)\n","      continue\n","\n","\n","  return candidate_features_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KepAZAiBXJAr"},"outputs":[],"source":["if ONE_EXAMPLE_TEST:\n","\n","  features_df = extract_features_for_uid(uid, SUBSETS_PATH, candidates_df, verbose=True, debug=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HgPVw8mCVMMQ"},"outputs":[],"source":["if ONE_EXAMPLE_TEST:\n","  features_df"]},{"cell_type":"markdown","metadata":{"id":"WCAFxxF3zAka"},"source":["## DISCARD CANDIDATES WITHOUT RELEVANT INFORMATION"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WG5yaN2fxrT6"},"outputs":[],"source":["if ONE_EXAMPLE_TEST:\n","  intersection = np.intersect1d(candidates_df.index, features_df.index)\n","\n","  filtered_candidates_df = candidates_df.loc[intersection]\n","\n","  filtered_candidates_df = pd.concat([filtered_candidates_df, features_df], axis=1)\n","  filtered_candidates_df[\"Class\"] = np.zeros_like(filtered_candidates_df.index)\n","  filtered_candidates_df"]},{"cell_type":"markdown","metadata":{"id":"5rlHF667ynH6"},"source":["## FIND TRUE POSITIVE CANDIDATES"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ISE24fwyMs1A"},"outputs":[],"source":["if ONE_EXAMPLE_TEST:\n","  neighborhood_th = 12\n","\n","  hit_indexes, hit_indexes_dict = find_neighborhood_indices(candidates_df, convert_annotation_df(annotations_by_uid(uid, ANNOTATIONS_DF)), neighborhood_th, verbose=True)\n","  filtered_candidates_df.loc[hit_indexes, \"Class\"] = np.full((len(hit_indexes)), 1)\n","\n","  print(f\"Hit indexes count: {len(hit_indexes)} --- Positive class count: {len(filtered_candidates_df[filtered_candidates_df['Class'] == 1])} \")\n","\n","  filtered_candidates_df[filtered_candidates_df[\"Class\"] == 1]\n","\n","  print(hit_indexes_dict)"]},{"cell_type":"markdown","metadata":{"id":"bQXahXH10Gdb"},"source":["## EXTRACT TO CSV TO TRAIN MODEL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FPICeYS20MFa"},"outputs":[],"source":["def get_trainable_candidate_features(uid, subsets_path, annotations_df, neighborhood_th = 12, save_csv_path=None, verbose=False, debug=False):\n","\n","  # SAFEGUARD NOT TO RUN UNNECESSARY OPERATIONS\n","  if not len(annotations_by_uid(uid, annotations_df)):\n","    raise Exception('NO ANNOTATIONS FOR THIS CASE')\n","\n","  subset = None\n","  for subset_dir in os.listdir(subsets_path):\n","    filenames = os.listdir(os.path.join(subsets_path, subset_dir)) # filenames in subset directory\n","    filenames = list(map(lambda x: os.path.splitext(x)[0], filenames)) # filename w/o extension\n","    filenames = set(filenames) # unique filenames (because .mhd and .raw have same filename)\n","    if uid in filenames:\n","      subset = subset_dir\n","      break # if found -> stop\n","\n","  if subset is None:\n","    raise FileNotFoundError(\"No image under this uid\")\n","\n","  img_path = os.path.join(subsets_path, subset, f\"{uid}.mhd\") # .mhd path\n","  mhd_img = sitk.ReadImage(img_path) # SimpleITK object\n","  img_3d = sitk.GetArrayFromImage(mhd_img)\n","  img_3d[img_3d < AIR_TH] = AIR_TH # Normalize minimum intensity value\n","\n","  print(f\"[GET CANDIDATES] - {uid}\")\n","  candidates_df = get_candidates_for_uid(uid, subsets_path, img_3d=img_3d, annotations_df=annotations_df, verbose=verbose)\n","  print(f\"[GET CANDIDATES] - {len(candidates_df)} candidates for {uid}\")\n","\n","  print(f\"[GET CANDIDATE FEATURES] - {uid}\")\n","  features_df = extract_features_for_uid(uid, subsets_path, candidates_df, img_3d=img_3d, verbose=verbose, debug=debug)\n","  print(f\"[GET CANDIDATE FEATURES] - {len(features_df)} candidates {uid}\")\n","\n","  intersection = np.intersect1d(candidates_df.index, features_df.index)\n","\n","  filtered_candidates_df = candidates_df.loc[intersection]\n","\n","  filtered_candidates_df = pd.concat([filtered_candidates_df, features_df], axis=1)\n","  filtered_candidates_df[\"Class\"] = np.zeros_like(filtered_candidates_df.index)\n","\n","  simple_annotations_df = convert_annotation_df(annotations_by_uid(uid, annotations_df))\n","\n","  print(f\"[GET TRUE POSITIVE CANDIDATES] - {uid}\")\n","  hit_indexes, hit_indexes_dict = find_neighborhood_indices(filtered_candidates_df, simple_annotations_df, neighborhood_th, verbose=verbose)\n","  filtered_candidates_df.loc[hit_indexes, \"Class\"] = np.full((len(hit_indexes)), 1)\n","  print(f\"[GET TRUE POSITIVE CANDIDATES] - {len(hit_indexes)} TP candidates for {uid}\")\n","\n","  sensitivity = sensitivity_score(filtered_candidates_df, simple_annotations_df, verbose=verbose)\n","  print(f\"[GET TRUE POSITIVE CANDIDATES] - {sensitivity} sensitivity for {uid}\")\n","\n","  if save_csv_path is not None:\n","    if verbose:\n","      print(f\"[SAVE] - candidate features csv {uid}\")\n","    # CHANGE THIS LINE FOR FUTURE NOTEBOOKS WITH DIFFERENT PROPERTIES\n","    save_csv_path = filtered_candidates_df.to_csv(os.path.join(save_csv_path, subset, f\"{uid}_candidates.csv\"))\n","\n","  return filtered_candidates_df, (hit_indexes, hit_indexes_dict), sensitivity"]},{"cell_type":"markdown","metadata":{"id":"v8rhCw_esdS4"},"source":["## TEST"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42,"status":"ok","timestamp":1717180364184,"user":{"displayName":"Áron B. Gimesi","userId":"03233526977624235264"},"user_tz":-120},"id":"JF0nWmWtYcvN","outputId":"c53c0640-2ad5-4c8c-c0c1-b33d9fbd4602"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'1.3.6.1.4.1.14519.5.2.1.6279.6001.139595277234735528205899724196', '1.3.6.1.4.1.14519.5.2.1.6279.6001.287966244644280690737019247886', '1.3.6.1.4.1.14519.5.2.1.6279.6001.152684536713461901635595118048', '1.3.6.1.4.1.14519.5.2.1.6279.6001.270390050141765094612147226290', '1.3.6.1.4.1.14519.5.2.1.6279.6001.259543921154154401875872845498', '1.3.6.1.4.1.14519.5.2.1.6279.6001.200558451375970945040979397866', '1.3.6.1.4.1.14519.5.2.1.6279.6001.616033753016904899083676284739', '1.3.6.1.4.1.14519.5.2.1.6279.6001.113697708991260454310623082679', '1.3.6.1.4.1.14519.5.2.1.6279.6001.222087811960706096424718056430', '1.3.6.1.4.1.14519.5.2.1.6279.6001.179162671133894061547290922949', '1.3.6.1.4.1.14519.5.2.1.6279.6001.768276876111112560631432843476', '1.3.6.1.4.1.14519.5.2.1.6279.6001.169128136262002764211589185953', '1.3.6.1.4.1.14519.5.2.1.6279.6001.114218724025049818743426522343', '1.3.6.1.4.1.14519.5.2.1.6279.6001.334105754605642100456249422350', '1.3.6.1.4.1.14519.5.2.1.6279.6001.674809958213117379592437424616', '1.3.6.1.4.1.14519.5.2.1.6279.6001.134370886216012873213579659366', '1.3.6.1.4.1.14519.5.2.1.6279.6001.259018373683540453277752706262', '1.3.6.1.4.1.14519.5.2.1.6279.6001.184019785706727365023450012318', '1.3.6.1.4.1.14519.5.2.1.6279.6001.247769845138587733933485039556', '1.3.6.1.4.1.14519.5.2.1.6279.6001.145759169833745025756371695397', '1.3.6.1.4.1.14519.5.2.1.6279.6001.121824995088859376862458155637', '1.3.6.1.4.1.14519.5.2.1.6279.6001.128881800399702510818644205032', '1.3.6.1.4.1.14519.5.2.1.6279.6001.146603910507557786636779705509', '1.3.6.1.4.1.14519.5.2.1.6279.6001.179049373636438705059720603192', '1.3.6.1.4.1.14519.5.2.1.6279.6001.308183340111270052562662456038', '1.3.6.1.4.1.14519.5.2.1.6279.6001.183184435049555024219115904825', '1.3.6.1.4.1.14519.5.2.1.6279.6001.128059192202504367870633619224', '1.3.6.1.4.1.14519.5.2.1.6279.6001.952265563663939823135367733681', '1.3.6.1.4.1.14519.5.2.1.6279.6001.892375496445736188832556446335', '1.3.6.1.4.1.14519.5.2.1.6279.6001.503980049263254396021509831276', '1.3.6.1.4.1.14519.5.2.1.6279.6001.479402560265137632920333093071', '1.3.6.1.4.1.14519.5.2.1.6279.6001.336894364358709782463716339027', '1.3.6.1.4.1.14519.5.2.1.6279.6001.282512043257574309474415322775', '1.3.6.1.4.1.14519.5.2.1.6279.6001.655242448149322898770987310561', '1.3.6.1.4.1.14519.5.2.1.6279.6001.801945620899034889998809817499', '1.3.6.1.4.1.14519.5.2.1.6279.6001.888291896309937415860209787179', '1.3.6.1.4.1.14519.5.2.1.6279.6001.206539885154775002929031534291', '1.3.6.1.4.1.14519.5.2.1.6279.6001.458525794434429386945463560826', '1.3.6.1.4.1.14519.5.2.1.6279.6001.104562737760173137525888934217', '1.3.6.1.4.1.14519.5.2.1.6279.6001.935683764293840351008008793409', '1.3.6.1.4.1.14519.5.2.1.6279.6001.243094273518213382155770295147', '1.3.6.1.4.1.14519.5.2.1.6279.6001.162718361851587451505896742103', '1.3.6.1.4.1.14519.5.2.1.6279.6001.140527383975300992150799777603', '1.3.6.1.4.1.14519.5.2.1.6279.6001.663019255629770796363333877035', '1.3.6.1.4.1.14519.5.2.1.6279.6001.144943344795414353192059796098', '1.3.6.1.4.1.14519.5.2.1.6279.6001.186021279664749879526003668137', '1.3.6.1.4.1.14519.5.2.1.6279.6001.197987940182806628828566429132', '1.3.6.1.4.1.14519.5.2.1.6279.6001.183843376225716802567192412456', '1.3.6.1.4.1.14519.5.2.1.6279.6001.250397690690072950000431855143', '1.3.6.1.4.1.14519.5.2.1.6279.6001.861997885565255340442123234170', '1.3.6.1.4.1.14519.5.2.1.6279.6001.335866409407244673864352309754', '1.3.6.1.4.1.14519.5.2.1.6279.6001.111017101339429664883879536171', '1.3.6.1.4.1.14519.5.2.1.6279.6001.161073793312426102774780216551', '1.3.6.1.4.1.14519.5.2.1.6279.6001.106719103982792863757268101375', '1.3.6.1.4.1.14519.5.2.1.6279.6001.325164338773720548739146851679', '1.3.6.1.4.1.14519.5.2.1.6279.6001.168605638657404145360275453085', '1.3.6.1.4.1.14519.5.2.1.6279.6001.171919524048654494439256263785', '1.3.6.1.4.1.14519.5.2.1.6279.6001.310395752124284049604069960014', '1.3.6.1.4.1.14519.5.2.1.6279.6001.163994693532965040247348251579', '1.3.6.1.4.1.14519.5.2.1.6279.6001.272961322147784625028175033640', '1.3.6.1.4.1.14519.5.2.1.6279.6001.193808128386712859512130599234', '1.3.6.1.4.1.14519.5.2.1.6279.6001.690929968028676628605553365896', '1.3.6.1.4.1.14519.5.2.1.6279.6001.208737629504245244513001631764', '1.3.6.1.4.1.14519.5.2.1.6279.6001.226456162308124493341905600418', '1.3.6.1.4.1.14519.5.2.1.6279.6001.275766318636944297772360944907', '1.3.6.1.4.1.14519.5.2.1.6279.6001.326057189095429101398977448288', '1.3.6.1.4.1.14519.5.2.1.6279.6001.193408384740507320589857096592', '1.3.6.1.4.1.14519.5.2.1.6279.6001.300271604576987336866436407488', '1.3.6.1.4.1.14519.5.2.1.6279.6001.108231420525711026834210228428', '1.3.6.1.4.1.14519.5.2.1.6279.6001.561458563853929400124470098603', '1.3.6.1.4.1.14519.5.2.1.6279.6001.173106154739244262091404659845', '1.3.6.1.4.1.14519.5.2.1.6279.6001.168037818448885856452592057286', '1.3.6.1.4.1.14519.5.2.1.6279.6001.315214756157389122376518747372', '1.3.6.1.4.1.14519.5.2.1.6279.6001.970264865033574190975654369557', '1.3.6.1.4.1.14519.5.2.1.6279.6001.100684836163890911914061745866', '1.3.6.1.4.1.14519.5.2.1.6279.6001.162207236104936931957809623059', '1.3.6.1.4.1.14519.5.2.1.6279.6001.161002239822118346732951898613', '1.3.6.1.4.1.14519.5.2.1.6279.6001.162901839201654862079549658100', '1.3.6.1.4.1.14519.5.2.1.6279.6001.231002159523969307155990628066', '1.3.6.1.4.1.14519.5.2.1.6279.6001.756684168227383088294595834066', '1.3.6.1.4.1.14519.5.2.1.6279.6001.314789075871001236641548593165', '1.3.6.1.4.1.14519.5.2.1.6279.6001.802595762867498341201607992711', '1.3.6.1.4.1.14519.5.2.1.6279.6001.286647622786041008124419915089', '1.3.6.1.4.1.14519.5.2.1.6279.6001.309672797925724868457151381131', '1.3.6.1.4.1.14519.5.2.1.6279.6001.331211682377519763144559212009', '1.3.6.1.4.1.14519.5.2.1.6279.6001.910607280658963002048724648683', '1.3.6.1.4.1.14519.5.2.1.6279.6001.216652640878960522552873394709', '1.3.6.1.4.1.14519.5.2.1.6279.6001.306948744223170422945185006551', '1.3.6.1.4.1.14519.5.2.1.6279.6001.231834776365874788440767645596'}\n"]}],"source":["print(UIDS)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PgUJuhsa0WV7"},"outputs":[],"source":["if ONE_EXAMPLE_TEST:\n","  uid = \"1.3.6.1.4.1.14519.5.2.1.6279.6001.511347030803753100045216493273\"\n","\n","  candidate_features, hit_indexes_tuple, sensitivity = get_trainable_candidate_features(\n","      uid,\n","      SUBSETS_PATH,\n","      ANNOTATIONS_DF,\n","      neighborhood_th=12,\n","      save_csv_path=SAVE_FOLDER_PATH,\n","      verbose=False,\n","      debug=False\n","  )\n","\n","  print(f\"sensitivity: {sensitivity}\")\n","\n","  print(f\"annotations:\")\n","  print(convert_annotation_df(annotations_by_uid(uid, ANNOTATIONS_DF)))\n","\n","  print(f\"hit_indexes for annotation indexes: {hit_indexes_tuple[1]}\")\n","  print(f\"hit_indexes: {hit_indexes_tuple[0]}\")\n"]},{"cell_type":"markdown","metadata":{"id":"1MJeHEui9Okx"},"source":["# RUN FOR ALL CASES IN SUBSET"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":314,"status":"ok","timestamp":1717181715831,"user":{"displayName":"Áron B. Gimesi","userId":"03233526977624235264"},"user_tz":-120},"id":"7n7lc72lWCjN","outputId":"231ff745-83f0-4f78-a1cc-ae3fa99abc3c"},"outputs":[{"output_type":"stream","name":"stdout","text":["[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.287966244644280690737019247886\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.270390050141765094612147226290\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.259543921154154401875872845498\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.616033753016904899083676284739\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.179162671133894061547290922949\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.768276876111112560631432843476\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.169128136262002764211589185953\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.114218724025049818743426522343\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.334105754605642100456249422350\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.674809958213117379592437424616\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.134370886216012873213579659366\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.247769845138587733933485039556\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.145759169833745025756371695397\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.121824995088859376862458155637\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.128881800399702510818644205032\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.179049373636438705059720603192\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.183184435049555024219115904825\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.128059192202504367870633619224\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.952265563663939823135367733681\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.892375496445736188832556446335\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.503980049263254396021509831276\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.479402560265137632920333093071\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.336894364358709782463716339027\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.282512043257574309474415322775\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.655242448149322898770987310561\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.801945620899034889998809817499\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.206539885154775002929031534291\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.458525794434429386945463560826\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.104562737760173137525888934217\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.935683764293840351008008793409\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.243094273518213382155770295147\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.162718361851587451505896742103\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.663019255629770796363333877035\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.183843376225716802567192412456\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.250397690690072950000431855143\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.861997885565255340442123234170\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.106719103982792863757268101375\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.325164338773720548739146851679\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.168605638657404145360275453085\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.310395752124284049604069960014\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.163994693532965040247348251579\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.272961322147784625028175033640\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.193808128386712859512130599234\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.690929968028676628605553365896\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.208737629504245244513001631764\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.226456162308124493341905600418\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.275766318636944297772360944907\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.326057189095429101398977448288\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.108231420525711026834210228428\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.561458563853929400124470098603\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.173106154739244262091404659845\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.168037818448885856452592057286\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.315214756157389122376518747372\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.970264865033574190975654369557\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.162901839201654862079549658100\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.231002159523969307155990628066\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.756684168227383088294595834066\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.314789075871001236641548593165\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.309672797925724868457151381131\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.910607280658963002048724648683\n","_____________________________________\n","\n","[SKIP] - features have already been extracted for case: 1.3.6.1.4.1.14519.5.2.1.6279.6001.306948744223170422945185006551\n","_____________________________________\n","\n"]}],"source":["neighborhood_th = 12\n","\n","result_dict = {}\n","\n","for i, uid in enumerate(UIDS):\n","  result_dict[uid] = {}\n","\n","  # If there is no annotation for a case skip it\n","  if not len(annotations_by_uid(uid, ANNOTATIONS_DF)):\n","    continue\n","\n","  # Check if feature extraction file already exists\n","  save_folder_subset_path = os.path.join(SAVE_FOLDER_PATH, SUBSET)\n","  save_filename =  f\"{uid}_candidates.csv\"\n","  features_file_path = os.path.join(save_folder_subset_path, save_filename)\n","  already_extracted = os.listdir(save_folder_subset_path)\n","\n","  # If feature_extraction file already exists skip it\n","  if save_filename in os.listdir(save_folder_subset_path):\n","    print(f\"[SKIP] - features have already been extracted for case: {uid}\")\n","    print(\"_____________________________________\\n\")\n","\n","    continue\n","\n","  print(f\"[START] - processing case {uid}\")\n","  candidate_features, hit_indexes, sensitivity = get_trainable_candidate_features(\n","      uid,\n","      SUBSETS_PATH,\n","      ANNOTATIONS_DF,\n","      neighborhood_th=neighborhood_th,\n","      save_csv_path=SAVE_FOLDER_PATH,\n","      verbose=False,\n","      debug=False\n","  )\n","\n","\n","  result_dict[uid][\"sensitivity\"] = sensitivity\n","  result_dict[uid][\"hit_indexes\"] = hit_indexes\n","\n","  print(f\"[DONE] - processing case {uid}\")\n","  print(\"\\n_____________________________________\\n\")\n","\n","  # if read_in_count > 5:\n","  #   break\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":227,"status":"ok","timestamp":1717181723622,"user":{"displayName":"Áron B. Gimesi","userId":"03233526977624235264"},"user_tz":-120},"id":"2hQYWveh4H5b","outputId":"c8eb2475-3e12-4dbc-8867-302f3ccb104d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'1.3.6.1.4.1.14519.5.2.1.6279.6001.139595277234735528205899724196': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.287966244644280690737019247886': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.152684536713461901635595118048': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.270390050141765094612147226290': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.259543921154154401875872845498': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.200558451375970945040979397866': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.616033753016904899083676284739': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.113697708991260454310623082679': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.222087811960706096424718056430': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.179162671133894061547290922949': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.768276876111112560631432843476': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.169128136262002764211589185953': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.114218724025049818743426522343': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.334105754605642100456249422350': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.674809958213117379592437424616': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.134370886216012873213579659366': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.259018373683540453277752706262': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.184019785706727365023450012318': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.247769845138587733933485039556': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.145759169833745025756371695397': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.121824995088859376862458155637': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.128881800399702510818644205032': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.146603910507557786636779705509': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.179049373636438705059720603192': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.308183340111270052562662456038': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.183184435049555024219115904825': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.128059192202504367870633619224': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.952265563663939823135367733681': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.892375496445736188832556446335': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.503980049263254396021509831276': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.479402560265137632920333093071': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.336894364358709782463716339027': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.282512043257574309474415322775': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.655242448149322898770987310561': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.801945620899034889998809817499': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.888291896309937415860209787179': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.206539885154775002929031534291': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.458525794434429386945463560826': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.104562737760173137525888934217': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.935683764293840351008008793409': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.243094273518213382155770295147': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.162718361851587451505896742103': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.140527383975300992150799777603': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.663019255629770796363333877035': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.144943344795414353192059796098': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.186021279664749879526003668137': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.197987940182806628828566429132': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.183843376225716802567192412456': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.250397690690072950000431855143': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.861997885565255340442123234170': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.335866409407244673864352309754': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.111017101339429664883879536171': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.161073793312426102774780216551': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.106719103982792863757268101375': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.325164338773720548739146851679': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.168605638657404145360275453085': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.171919524048654494439256263785': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.310395752124284049604069960014': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.163994693532965040247348251579': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.272961322147784625028175033640': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.193808128386712859512130599234': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.690929968028676628605553365896': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.208737629504245244513001631764': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.226456162308124493341905600418': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.275766318636944297772360944907': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.326057189095429101398977448288': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.193408384740507320589857096592': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.300271604576987336866436407488': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.108231420525711026834210228428': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.561458563853929400124470098603': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.173106154739244262091404659845': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.168037818448885856452592057286': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.315214756157389122376518747372': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.970264865033574190975654369557': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.100684836163890911914061745866': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.162207236104936931957809623059': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.161002239822118346732951898613': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.162901839201654862079549658100': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.231002159523969307155990628066': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.756684168227383088294595834066': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.314789075871001236641548593165': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.802595762867498341201607992711': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.286647622786041008124419915089': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.309672797925724868457151381131': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.331211682377519763144559212009': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.910607280658963002048724648683': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.216652640878960522552873394709': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.306948744223170422945185006551': {},\n"," '1.3.6.1.4.1.14519.5.2.1.6279.6001.231834776365874788440767645596': {}}"]},"metadata":{},"execution_count":26}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"error","timestamp":1717181629904,"user":{"displayName":"Áron B. Gimesi","userId":"03233526977624235264"},"user_tz":-120},"id":"mpif3X7Bs9u-","outputId":"addbaa6a-3443-4f9a-8a89-ac75358cd478"},"outputs":[{"ename":"Exception","evalue":"PLEASE STOP HERE","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-0bedd4b9250c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PLEASE STOP HERE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mException\u001b[0m: PLEASE STOP HERE"]}],"source":["raise Exception(\"PLEASE STOP HERE\")"]},{"cell_type":"markdown","metadata":{"id":"oLj7WojQQfke"},"source":["# ASYNC RUN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aalK1WEH2pK3"},"outputs":[],"source":["import os\n","from concurrent.futures import ThreadPoolExecutor\n","\n","def process_uid(uid, annotations_df, subsets_path, save_folder_path, subset, neighborhood_th):\n","    result = {}\n","    # If there is no annotation for a case skip it\n","    if not len(annotations_by_uid(uid, annotations_df)):\n","        return uid, None\n","\n","    # Check if feature extraction file already exists\n","    save_folder_subset_path = os.path.join(save_folder_path, subset)\n","    save_filename = f\"{uid}_candidates.csv\"\n","    features_file_path = os.path.join(save_folder_subset_path, save_filename)\n","    already_extracted = os.listdir(save_folder_subset_path)\n","\n","    # If feature_extraction file already exists skip it\n","    if save_filename in already_extracted:\n","        print(f\"[SKIP] - features have already been extracted for case: {uid}\")\n","        print(\"_____________________________________\\n\")\n","        return uid, None\n","\n","    print(f\"[START] - processing case {uid}\")\n","    candidate_features, hit_indexes, sensitivity = get_trainable_candidate_features(\n","        uid,\n","        subsets_path,\n","        annotations_df,\n","        neighborhood_th=neighborhood_th,\n","        save_csv_path=save_folder_path,\n","        verbose=False,\n","        debug=False\n","    )\n","\n","    result[\"sensitivity\"] = sensitivity\n","    result[\"hit_indexes\"] = hit_indexes\n","\n","    print(f\"[DONE] - processing case {uid}\")\n","    print(\"\\n_____________________________________\\n\")\n","\n","    return uid, result\n","\n","def extract_features_concurrently(uids, annotations_df, subsets_path, save_folder_path, subset, neighborhood_th, max_workers=3):\n","    result_dict = {}\n","\n","    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n","        futures = {executor.submit(process_uid, uid, annotations_df, subsets_path, save_folder_path, subset, neighborhood_th): uid for uid in uids}\n","\n","        for future in futures:\n","            uid, result = future.result()\n","            if result is not None:\n","                result_dict[uid] = result\n","\n","    return result_dict\n","\n","result_dict = extract_features_concurrently(UIDS, ANNOTATIONS_DF, SUBSETS_PATH, SAVE_FOLDER_PATH, SUBSET, neighborhood_th, max_workers=3)\n"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}