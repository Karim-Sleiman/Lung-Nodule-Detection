{"cells":[{"cell_type":"markdown","metadata":{"id":"hhJZ2xVMOtf3"},"source":["# CONNECT TO DRIVE AND IMPORTS"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9672,"status":"ok","timestamp":1718044913365,"user":{"displayName":"Áron B. Gimesi","userId":"03233526977624235264"},"user_tz":-120},"id":"SbRlfrRqOphP","outputId":"9a937621-442c-4e11-adfb-db43ce9ec93b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: SimpleITK in /usr/local/lib/python3.10/dist-packages (2.3.1)\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["!pip install SimpleITK\n","\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1718044913365,"user":{"displayName":"Áron B. Gimesi","userId":"03233526977624235264"},"user_tz":-120},"id":"yz2zWEPAOqAu"},"outputs":[],"source":["import os\n","import sys\n","import inspect\n","import numpy as np\n","import pandas as pd\n","import cv2 as cv\n","import matplotlib.pyplot as plt\n","\n","import SimpleITK as sitk\n","\n","from google.colab.patches import cv2_imshow"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1718044913366,"user":{"displayName":"Áron B. Gimesi","userId":"03233526977624235264"},"user_tz":-120},"id":"5ebsc6oKOsdn","outputId":"f01b8726-18eb-4bc0-c884-416917774dd2"},"outputs":[{"name":"stdout","output_type":"stream","text":["['annotations_by_uid', 'binarize_lung', 'binarize_lung_3d', 'binary_closing', 'binary_dilation', 'binary_erosion', 'binary_fill_holes', 'binary_opening', 'center_of_mass', 'clear_border', 'convert_annotation_df', 'convert_annotation_df_with_uid', 'create_3d_mask', 'create_annotations_mask', 'create_annotations_mask_by_uid', 'create_patch', 'debugger', 'draw_ellipsoid', 'find_by_uid', 'find_neighborhood_indices', 'find_neighborhood_indices_more_precise', 'get_slice_candidates', 'get_slice_candidates_old', 'get_slices', 'get_uids', 'img_by_uid', 'masked_annotations_by_uid', 'masked_annotations_with_info_by_uid', 'meta_by_uid', 'norm2float', 'norm2uint16', 'norm2uint8', 'normalize_intensity', 'plot_slices', 'process_slice_candidates', 'process_slice_candidates_old', 'process_slices', 'remove_non_central_objects', 'sensitivity_score', 'sensitivity_score_more_precise', 'show_3_images', 'subset_by_uid', 'unwanted_object_filter']\n"]}],"source":["path = \"/content/drive/Shareddrives/IA DL_project/ML IA/nb_Aron\"\n","\n","if path not in sys.path:\n","  sys.path.append(path)\n","\n","import luna_module\n","from luna_module import *\n","\n","# List all function names in the luna_module\n","function_names = [name for name, obj in inspect.getmembers(luna_module) if inspect.isfunction(obj)]\n","print(function_names)"]},{"cell_type":"markdown","metadata":{"id":"zSJQqqvR9ttU"},"source":["# CONSTANTS"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1718044913366,"user":{"displayName":"Áron B. Gimesi","userId":"03233526977624235264"},"user_tz":-120},"id":"8-bI0_bF9wKY"},"outputs":[],"source":["LUNA_PATH = os.path.join(os.getcwd(), \"drive\", \"Shareddrives\", \"IA DL_project\", \"ML IA\", \"LUNA16\")\n","\n","SUBSET = \"subset9\"\n","\n","SUBSETS_PATH = os.path.join(LUNA_PATH, \"subsets\") # path for subsets folder\n","SUBSET_PATH =  os.path.join(SUBSETS_PATH, SUBSET) # path for subsets folder\n","SAVE_FOLDER_PATH = os.path.join(LUNA_PATH, \"feature_extractions\", \"nomask_noclip_gabor_4haar\")\n","\n","\n","ANNOTATIONS_DF = pd.read_csv(os.path.join(LUNA_PATH, f\"{SUBSET}_annotations_expanded.csv\"), index_col=\"Unnamed: 0\")\n","\n","SUBSETS = os.listdir(SUBSET_PATH) # subset folders present\n","FILENAMES = os.listdir(SUBSET_PATH)\n","UIDS = set(map(lambda filename: os.path.splitext(filename)[0], FILENAMES))"]},{"cell_type":"markdown","metadata":{"id":"V5VgEWgeICWD"},"source":["# EXTRACT FEATURES"]},{"cell_type":"markdown","metadata":{"id":"jKATcIUZkovX"},"source":["## HAAR FEATURES"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1718044913366,"user":{"displayName":"Áron B. Gimesi","userId":"03233526977624235264"},"user_tz":-120},"id":"kFPm9O6gJ6nP"},"outputs":[],"source":["import numpy as np\n","import cv2 as cv\n","import pandas as pd\n","\n","def create_center_surround_kernel(size, inner_size):\n","    kernel = np.zeros((size, size), dtype=np.float32)\n","\n","    # Define the start and end points of the inner region\n","    start = (size - inner_size) // 2\n","    end = start + inner_size\n","\n","    # Set the central region to +1\n","    kernel[start:end, start:end] = 1\n","\n","    # Set the surround region to -1\n","    kernel[:start, :] = -1\n","    kernel[end:, :] = -1\n","    kernel[:, :start] = -1\n","    kernel[:, end:] = -1\n","\n","    return kernel\n","\n","def create_four_rectangle_kernel(size):\n","    kernel = np.zeros((size, size), dtype=np.float32)\n","\n","    half_size = size // 2\n","\n","    # Top-left and bottom-right rectangles (+1)\n","    kernel[:half_size, :half_size] = 1\n","    kernel[half_size:, half_size:] = 1\n","\n","    # Top-right and bottom-left rectangles (-1)\n","    kernel[:half_size, half_size:] = -1\n","    kernel[half_size:, :half_size] = -1\n","\n","    return kernel\n","\n","def extract_haar_features(p_haar, kernel):\n","    # Ensure the patch and kernel are float32\n","    p_haar = p_haar.astype(np.float32)\n","    kernel = kernel.astype(np.float32)\n","\n","    # Apply the kernel to the patch using convolution\n","    filtered_patch = cv.filter2D(p_haar, -1, kernel)\n","    return filtered_patch\n","\n","def multi_scale_haar_features(p_haar, kernel, scales):\n","    haar_features_list = []\n","\n","    for scale in scales:\n","        # Resize the patch\n","        p_haar = p_haar.astype('float32')\n","\n","        row_th = int(p_haar.shape[0]*scale)\n","        col_th = int(p_haar.shape[1]*scale)\n","\n","        scaled_patch = cv.resize(p_haar, (row_th, col_th))\n","        # scaled_patch = cv.resize(p_haar, (row_th, col_th), fx=scale, fy=scale, interpolation=cv.INTER_LINEAR)\n","\n","        # Ensure the scaled patch is valid\n","        if scaled_patch.size == 0:\n","            raise ValueError(f\"Scaled patch is empty at scale {scale}\")\n","\n","        # Extract Haar features from the scaled patch\n","        haar_features = extract_haar_features(scaled_patch, kernel)\n","        haar_features_list.append(haar_features)\n","\n","        # Find the location of the maximum response\n","        min_val, max_val, min_loc, max_loc = cv.minMaxLoc(haar_features)\n","\n","    return haar_features_list\n","\n","\n","# Function to pad a feature map to a specified size\n","def pad_feature_map(feature_map, target_size):\n","    padded_feature_map = np.zeros((target_size, target_size), dtype=np.float32)\n","    current_size = feature_map.shape[0]\n","\n","    # Calculate padding offsets\n","    offset = (target_size - current_size) // 2\n","\n","    # Place the feature map in the center of the padded array\n","    padded_feature_map[offset:offset + current_size, offset:offset + current_size] = feature_map\n","\n","    return padded_feature_map\n","\n","def combine_and_average_feature_maps(feature_maps, target_size):\n","    padded_feature_maps = [pad_feature_map(fm, target_size) for fm in feature_maps]\n","    combined_feature_map = np.mean(padded_feature_maps, axis=0)\n","\n","    return combined_feature_map\n","\n","def combine_and_weighted_average_feature_maps(feature_maps, target_size, weights):\n","    weighted_sum = np.zeros((target_size, target_size), dtype=np.float32)\n","    total_weight = 0\n","\n","    for feature_map, weight in zip(feature_maps, weights):\n","        padded_feature_map = pad_feature_map(feature_map, target_size)\n","        weighted_sum += padded_feature_map * weight\n","        total_weight += weight\n","\n","    combined_feature_map = weighted_sum / total_weight\n","    return combined_feature_map\n","\n","def custom_pooling(feature_map, kernel_size, pooling_type='avg'):\n","    h, w = feature_map.shape\n","    pooled_map = np.zeros((h // kernel_size, w // kernel_size), dtype=np.float32)\n","\n","    for i in range(0, h, kernel_size):\n","        for j in range(0, w, kernel_size):\n","            patch = feature_map[i:i + kernel_size, j:j + kernel_size]\n","            if pooling_type == 'avg':\n","                pooled_map[i // kernel_size, j // kernel_size] = np.mean(patch)\n","            elif pooling_type == 'max':\n","                pooled_map[i // kernel_size, j // kernel_size] = np.max(patch)\n","            else:\n","                raise ValueError(\"Pooling type must be 'avg' or 'max'\")\n","\n","    return pooled_map\n","\n","def haar_pooled(patch, kerneltype, target_size = 48):\n","    \"\"\"\n","    Perform multi-scale Haar feature extraction and pooling on the input patch.\n","\n","    Args:\n","        p_haar (np.ndarray): The input patch.\n","        kerneltype (str): Type of Haar kernel ('cs' for center-surround, 'fr' for four-rectangle).\n","        target_size (int): Desired size of patch rectangle\n","\n","    Returns:\n","        tuple: Pooled feature maps (average pooling, max pooling).\n","    \"\"\"\n","    # Initialize Haar parameters\n","    kernel_size_cs = 8\n","    inner_size_cs = 4\n","    size_fr = 8\n","\n","    # Define scales based on patch size\n","    scales = [1, 0.875, 0.75, 0.625, 0.5, 0.375, 0.25, 0.125] if patch.shape[0] == 64 else [1, 0.875, 0.75, 0.625, 0.5, 0.375, 0.25]\n","\n","    # Calculate weights for the scales\n","    weights = [1 / scale for scale in scales]\n","    weights = [w / sum(weights) for w in weights]\n","\n","    # Pooling parameters\n","    kernel_poolsize = 8\n","\n","    # Select the appropriate kernel\n","    if kerneltype == 'cs':\n","        kernel = create_center_surround_kernel(kernel_size_cs, inner_size_cs)\n","    elif kerneltype == 'fr':\n","        kernel = create_four_rectangle_kernel(size_fr)\n","    else:\n","        raise ValueError(\"Kernel type must be 'cs' (center-surround) or 'fr' (four-rectangle)\")\n","\n","    # Extract Haar features at multiple scales\n","    haar_features_list = multi_scale_haar_features(patch, kernel, scales)\n","\n","    # Combine and weight the feature maps\n","    combined_feature_map = combine_and_weighted_average_feature_maps(haar_features_list, target_size, weights)\n","    # combined_feature_map = combine_and_average_feature_maps(haar_features_list, target_size)\n","\n","    # Perform custom pooling\n","    pooled_feature_map_avg = custom_pooling(combined_feature_map, kernel_poolsize, pooling_type='avg')\n","    pooled_feature_map_max = custom_pooling(combined_feature_map, kernel_poolsize, pooling_type='max')\n","\n","    return pooled_feature_map_avg, pooled_feature_map_max\n","\n","\n","# Example usage\n","# p_haar = #your patch\n","# pooled_avg, pooled_max = haar_pooled(p_haar, 'cs')\n"]},{"cell_type":"markdown","metadata":{"id":"IJ0duWIlOVWt"},"source":["## GABOR FEATURES"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1718044913366,"user":{"displayName":"Áron B. Gimesi","userId":"03233526977624235264"},"user_tz":-120},"id":"3QpGneVxOVLK"},"outputs":[],"source":["def create_gabor_kernels(ksize=32, sigma=4.0, lambd=10.0, gamma=0.5, psi=0):\n","    \"\"\"\n","    Create a set of Gabor kernels with different orientations and frequencies.\n","\n","    Args:\n","        ksize (int): Size of the Gabor kernel.\n","        sigma (float): Standard deviation of the Gaussian function.\n","        lambd (float): Wavelength of the sinusoidal factor.\n","        gamma (float): Spatial aspect ratio.\n","        psi (float): Phase offset.\n","\n","    Returns:\n","        list: A list of Gabor kernels.\n","    \"\"\"\n","    kernels = []\n","    for theta in np.arange(0, np.pi, np.pi / 8):  # 8 orientations\n","        kernel = cv.getGaborKernel((ksize, ksize), sigma, theta, lambd, gamma, psi, ktype=cv.CV_32F)\n","        kernels.append(kernel)\n","    return kernels\n","\n","def apply_gabor_kernels(patch, kernels):\n","    \"\"\"\n","    Apply Gabor kernels to a patch to extract features.\n","\n","    Args:\n","        patch (np.ndarray): The input image patch (64x64).\n","        kernels (list): A list of Gabor kernels.\n","\n","    Returns:\n","        np.ndarray: The combined Gabor feature vector.\n","    \"\"\"\n","    features = []\n","    for kernel in kernels:\n","        filtered_patch = cv.filter2D(patch, cv.CV_32F, kernel)\n","        features.append(filtered_patch)\n","\n","    # Combine features (e.g., by flattening and concatenating)\n","    combined_features = np.hstack([f.flatten() for f in features])\n","    return combined_features\n","\n","def apply_gabor_kernels_and_compute_mean(patch, kernels):\n","    \"\"\"\n","    Apply Gabor kernels to a patch and compute the mean feature map.\n","\n","    Args:\n","        patch (np.ndarray): The input image patch (64x64).\n","        kernels (list): A list of Gabor kernels.\n","\n","    Returns:\n","        np.ndarray: The mean Gabor feature map.\n","    \"\"\"\n","    feature_maps = []\n","    for kernel in kernels:\n","        filtered_patch = cv.filter2D(patch, cv.CV_32F, kernel)\n","        feature_maps.append(filtered_patch)\n","\n","    # Compute the mean feature map\n","    mean_feature_map = np.mean(feature_maps, axis=0)\n","    return mean_feature_map\n","\n","def custom_pooling(feature_map, kernel_size, pooling_type='avg'):\n","    h, w = feature_map.shape\n","    pooled_map = np.zeros((h // kernel_size, w // kernel_size), dtype=np.float32)\n","\n","    for i in range(0, h, kernel_size):\n","        for j in range(0, w, kernel_size):\n","            patch = feature_map[i:i + kernel_size, j:j + kernel_size]\n","            if pooling_type == 'avg':\n","                pooled_map[i // kernel_size, j // kernel_size] = np.mean(patch)\n","            elif pooling_type == 'max':\n","                pooled_map[i // kernel_size, j // kernel_size] = np.max(patch)\n","            else:\n","                raise ValueError(\"Pooling type must be 'avg' or 'max'\")\n","\n","    return pooled_map\n","\n","\n","def gabor_pooled(patch, kernel_poolsize=8):\n","    \"\"\"\n","    Apply Gabor filters to the patch, compute mean feature map, and perform pooling.\n","\n","    Args:\n","        patch (np.ndarray): The input image patch (64x64).\n","        kernel_poolsize (int): Size of the pooling kernel.\n","\n","    Returns:\n","        tuple: Pooled feature maps (average pooling, max pooling).\n","    \"\"\"\n","    # Create Gabor kernels\n","    kernels = create_gabor_kernels()\n","\n","    # Apply Gabor kernels and compute the mean feature map\n","    mean_feature_map = apply_gabor_kernels_and_compute_mean(patch, kernels)\n","\n","    # Perform custom pooling\n","    pooled_feature_map_avg = custom_pooling(mean_feature_map, kernel_poolsize, pooling_type='avg')\n","    pooled_feature_map_max = custom_pooling(mean_feature_map, kernel_poolsize, pooling_type='max')\n","\n","    return pooled_feature_map_avg, pooled_feature_map_max"]},{"cell_type":"markdown","metadata":{"id":"9wJubs0Zks6I"},"source":["## FEATURE EXTRACT FUNCTION"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":564,"status":"ok","timestamp":1718044913919,"user":{"displayName":"Áron B. Gimesi","userId":"03233526977624235264"},"user_tz":-120},"id":"heMRldzAKLVK"},"outputs":[],"source":["from skimage.feature import graycomatrix, graycoprops, local_binary_pattern\n","from skimage.measure import regionprops, label\n","from skimage import io, color\n","\n","def debugger(img, title=None):\n","    if title is not None:\n","        print(title)\n","    print(np.unique(img))\n","    plt.imshow(img, cmap=\"gray\")\n","    plt.show()\n","\n","def mask_closest_object_to_center(binary_image, debug=False):\n","    def debugger(image, step):\n","        # Debug function placeholder; replace with actual debug functionality if needed\n","        print(f\"{step}: {image}\")\n","\n","    if debug:\n","        debugger(binary_image, \"mask_closest_object_to_center 1st step\")\n","\n","    # Label connected components\n","    labeled_image = label(binary_image)\n","\n","    if debug:\n","        debugger(labeled_image, \"mask_closest_object_to_center 2nd step\")\n","\n","    # Get image center\n","    image_center = tuple(np.array(binary_image.shape) // 2)\n","\n","    # Check if the center pixel is within any labeled object\n","    center_label = labeled_image[image_center]\n","    if center_label \u003e 0:\n","        # Create a mask for the object containing the center pixel\n","        center_mask = np.zeros_like(binary_image, dtype=bool)\n","        center_mask[labeled_image == center_label] = 1\n","\n","        if debug:\n","            debugger(center_mask, \"mask_closest_object_to_center - center object found\")\n","\n","        center_mask = binary_fill_holes(center_mask)\n","        center_mask = binary_dilation(center_mask, morphology.disk(1))\n","\n","        return center_mask.astype(float)\n","\n","    region_properties = regionprops(labeled_image)\n","    if not len(region_properties):\n","        raise Exception(\"No labels were found\")\n","\n","    # Initialize variables to track the closest object\n","    min_distance = float('inf')\n","    closest_label = None\n","\n","    # Iterate through each labeled object\n","    for region in region_properties:\n","        # Compute the centroid of the object\n","        centroid = region.centroid\n","\n","        # Calculate the Euclidean distance to the image center\n","        distance = np.linalg.norm(np.array(centroid) - image_center)\n","\n","        # Update the closest object if this one is closer\n","        if distance \u003c min_distance:\n","            min_distance = distance\n","            closest_label = region.label\n","\n","    # Create a mask for the closest object\n","    closest_mask = np.zeros_like(binary_image, dtype=bool)\n","    closest_mask[labeled_image == closest_label] = 1\n","\n","    if debug:\n","        debugger(closest_mask, \"mask_closest_object_to_center 3rd step\")\n","\n","    closest_mask = binary_fill_holes(closest_mask)\n","    closest_mask = binary_dilation(closest_mask, morphology.disk(1))\n","\n","    if debug:\n","        debugger(closest_mask, \"mask_closest_object_to_center 4th step\")\n","\n","    return closest_mask.astype(float)\n","\n","from skimage.feature import hog\n","from skimage import exposure\n","\n","def extract_hog_features(patch):\n","    \"\"\"\n","    Extract Histogram of Oriented Gradients (HOG) features from a given patch.\n","\n","    Args:\n","        patch (np.ndarray): The input image patch (64x64).\n","\n","    Returns:\n","        tuple: HOG features and HOG image.\n","    \"\"\"\n","    # Ensure the patch is of type float32\n","    patch = patch.astype(np.float32)\n","\n","    # Compute HOG features\n","    hog_features = hog(patch,\n","                        orientations=9,\n","                        pixels_per_cell=(8, 8),\n","                        cells_per_block=(2, 2),\n","                        visualize=False)\n","\n","    return hog_features\n","\n","def extract_features(nodule_roi, roi_mask, thresholding=\"otsu\", clip=False, verbose=False, debug=False):\n","\n","  if debug:\n","    debugger(nodule_roi, \"nodule_roi\")\n","    debugger(roi_mask, \"roi_mask\")\n","\n","\n","  if clip:\n","    n_th = -200\n","    p_th = 200\n","\n","    nodule_roi[nodule_roi\u003cn_th] = AIR_TH\n","    nodule_roi[nodule_roi\u003ep_th] = AIR_TH\n","    if len(nodule_roi[nodule_roi!=AIR_TH].nonzero()[0]) \u003c 3: # \u003e3 because if there is only 2 nonzero pix it should discard\n","      raise Exception(\"After clipping no information was left\")\n","\n","  # roi_mask = binary_dilation(roi_mask, morphology.disk(1))\n","  # if debug:\n","  #   debugger(roi_mask, \"roi_mask\")\n","\n","\n","\n","  ret, otsu_img = cv.threshold(norm2uint8(nodule_roi), 0, 255, cv.THRESH_BINARY + cv.THRESH_OTSU)\n","  ret, binary_th_img = cv.threshold(norm2uint8(nodule_roi), 127, 255, cv.THRESH_BINARY)\n","\n","  match (thresholding):\n","    case \"otsu\":\n","      th = otsu_img\n","    case \"binary\":\n","      th = binary_th_img\n","    case _:\n","      th = otsu_img\n","\n","  if debug:\n","    debugger(th, \"thresholded\")\n","\n","  th[roi_mask == False] = 0\n","\n","  if debug:\n","    debugger(th, \"thresholded+masked\")\n","\n","\n","  center_object_mask = mask_closest_object_to_center(th, debug)\n","  if debug:\n","    debugger(center_object_mask, \"center_object_mask\")\n","\n","  labeled_nodule = label(center_object_mask)\n","  if debug:\n","    debugger(labeled_nodule, \"labeled_nodule\")\n","\n","  props = regionprops(labeled_nodule)[0]\n","  if debug:\n","    print(props)\n","\n","  # Binary Shape features\n","  area = props.area\n","  if debug:\n","    print(f\"area: {area}\")\n","\n","  perimeter = props.perimeter\n","  if debug:\n","    print(f\"perimeter: {perimeter}\")\n","\n","  compactness = (perimeter ** 2) / area\n","  if debug:\n","    print(f\"compactness: {compactness}\")\n","\n","  eccentricity = props.eccentricity\n","  if debug:\n","    print(f\"eccentricity: {eccentricity}\")\n","\n","  major_axis_length = props.major_axis_length\n","  if debug:\n","    print(f\"major_axis_length: {major_axis_length}\")\n","\n","  minor_axis_length = props.minor_axis_length\n","  if debug:\n","    print(f\"minor_axis_length: {minor_axis_length}\")\n","\n","  solidity = props.solidity\n","  if debug:\n","    print(f\"solidity: {solidity}\")\n","\n","  extent = props.extent\n","  if debug:\n","    print(f\"extent: {extent}\")\n","\n","  # Texture features using GLCM\n","  nodule_roi_uint8 = norm2uint8(nodule_roi)\n","  glcm = graycomatrix(nodule_roi_uint8, distances=[1], angles=[0], levels=256, symmetric=True, normed=True)\n","\n","  contrast = graycoprops(glcm, 'contrast')[0, 0]\n","  if debug:\n","    print(f\"contrast: {contrast}\")\n","\n","  correlation = graycoprops(glcm, 'correlation')[0, 0]\n","  if debug:\n","    print(f\"correlation: {correlation}\")\n","\n","  energy = graycoprops(glcm, 'energy')[0, 0]\n","  if debug:\n","    print(f\"energy: {energy}\")\n","\n","  homogeneity = graycoprops(glcm, 'homogeneity')[0, 0]\n","  if debug:\n","    print(f\"homogeneity: {homogeneity}\")\n","\n","  # Intensity features\n","  mean_intensity = np.mean(nodule_roi)\n","  if debug:\n","    print(f\"mean_intensity: {mean_intensity}\")\n","\n","  std_intensity = np.std(nodule_roi)\n","  if debug:\n","    print(f\"std_intensity: {std_intensity}\")\n","\n","  skewness = np.mean(((nodule_roi - mean_intensity) / std_intensity) ** 3)\n","  if debug:\n","    print(f\"skewness: {skewness}\")\n","\n","  kurtosis = np.mean(((nodule_roi - mean_intensity) / std_intensity) ** 4) - 3\n","  if debug:\n","    print(f\"kurtosis: {kurtosis}\")\n","\n","  # LBP texture feature\n","  lbp = local_binary_pattern(nodule_roi, P=8, R=1, method='uniform')\n","  if debug:\n","    print(f\"lbp: {lbp}\")\n","\n","  lbp_hist, _ = np.histogram(lbp, bins=np.arange(0, 10), range=(0, 9))\n","  lbp_hist = lbp_hist / np.sum(lbp_hist)\n","  if debug:\n","    print(f\"lbp_hist: {lbp_hist}\")\n","\n","  # Haar features\n","  # Center sorround\n","  pooled_avg_cs, pooled_max_cs = haar_pooled(nodule_roi, 'cs')\n","  if debug:\n","    print(f\"pooled_avg_cs: {pooled_avg_cs}\")\n","    print(f\"pooled_max_cs: {pooled_max_cs}\")\n","  # hog = extract_hog_features(nodule_roi)\n","\n","  # Four rectangle\n","  pooled_avg_fr, pooled_max_fr = haar_pooled(nodule_roi, 'fr')\n","  if debug:\n","    print(f\"pooled_avg_fr: {pooled_avg_fr}\")\n","    print(f\"pooled_max_fr: {pooled_max_fr}\")\n","\n","  # Gabor features\n","  gabor_features_avg, gabor_features_max = gabor_pooled(nodule_roi)\n","  if debug:\n","    print(f\"gabor_features_avg: {gabor_features_avg}\")\n","    print(f\"gabor_features_max: {gabor_features_max}\")\n","\n","  # hog_features = hog(nodule_roi)\n","\n","  # Aggregate features into a dictionary or array\n","  feature_dict = {\n","      'area': area,\n","      'perimeter': perimeter,\n","      'compactness': compactness,\n","      'eccentricity': eccentricity,\n","      'major_axis_length': major_axis_length,\n","      'minor_axis_length': minor_axis_length,\n","      'solidity': solidity,\n","      'extent': extent,\n","      'contrast': contrast,\n","      'correlation': correlation,\n","      'energy': energy,\n","      'homogeneity': homogeneity,\n","      'mean_intensity': mean_intensity,\n","      'std_intensity': std_intensity,\n","      'skewness': skewness,\n","      'kurtosis': kurtosis,\n","      'lbp_hist': lbp_hist.tolist(),\n","      \"haar_avg_cs\": pooled_avg_cs.tolist(),\n","      \"haar_max_cs\": pooled_max_cs.tolist(),\n","      \"haar_avg_fr\": pooled_avg_fr.tolist(),\n","      \"haar_max_fr\": pooled_max_fr.tolist(),\n","      \"pooled_avg_cs\": pooled_avg_cs.tolist(),\n","      \"pooled_max_cs\": pooled_max_cs.tolist(),\n","      \"gabor_features_avg\": gabor_features_avg.tolist(),\n","      \"gabor_features_max\": gabor_features_max.tolist(),\n","\n","\n","      # 'hog': hog_features\n","  }\n","\n","\n","  # Unraveling/unpacking keys with lists in them\n","  expanded_features = {}\n","\n","  # Iterate over the original dictionary\n","  for key, value in feature_dict.items():\n","      if isinstance(value, list) and all(isinstance(i, list) for i in value):\n","          # If the value is a list of lists, unpack the sublists into new keys\n","          for i, sublist in enumerate(value, start=1):\n","              for j, v in enumerate(sublist, start=1):\n","                  new_key = f\"{key}{i}{j}\"\n","                  expanded_features[new_key] = v\n","      elif isinstance(value, list):\n","          # If the value is a list, unpack the list into new keys\n","          for i, v in enumerate(value, start=1):\n","              new_key = f\"{key}{i}\"\n","              expanded_features[new_key] = v\n","      else:\n","          # If the value is not a list of lists, keep the original key-value pair\n","          expanded_features[key] = value\n","\n","  return expanded_features"]},{"cell_type":"markdown","metadata":{"id":"7a4S_LETIB1d"},"source":["## VALIDATION"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":30,"status":"ok","timestamp":1718044913920,"user":{"displayName":"Áron B. Gimesi","userId":"03233526977624235264"},"user_tz":-120},"id":"01MWoDLFIE7V"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","def convert_annotation_df(annotations_df, verbose=False):\n","    node_coords = annotations_df[\"cartesian_coords(zyx)\"].apply(\n","        lambda x: np.array(x.replace(\"(\", \"\").replace(\")\", \"\").split(\",\")).astype(int)[::-1]\n","    )\n","    node_coords = np.array(node_coords.to_list()).reshape((len(node_coords.to_list()), 3))\n","\n","    # Create a new DataFrame to store the extracted coordinates\n","    node_coords_df = pd.DataFrame(columns=[\"x\", \"y\", \"z\"], index=annotations_df.index)\n","    node_coords_df[\"x\"] = node_coords[:, 0]\n","    node_coords_df[\"y\"] = node_coords[:, 1]\n","    node_coords_df[\"z\"] = node_coords[:, 2]\n","\n","    if verbose:\n","      print(f\"[ANNOTATIONS] - simplified\")\n","      print(node_coords_df)\n","\n","    return node_coords_df\n","\n","def find_neighborhood_indices(large_df, small_df, threshold=15, verbose=False):\n","    \"\"\"\n","    Find indices in the larger DataFrame where the coordinates are within a\n","    specified neighborhood distance from any row coordinates in the smaller DataFrame.\n","\n","    Parameters:\n","    large_df (pd.DataFrame): The larger DataFrame with 'x', 'y', 'z' columns.\n","    small_df (pd.DataFrame): The smaller DataFrame with 'x', 'y', 'z' columns.\n","    threshold (float): The neighborhood distance threshold. Default is 15.\n","\n","    Returns:\n","    np.ndarray: An array of indices from the larger DataFrame that are within the neighborhood.\n","    \"\"\"\n","    # Initialize an empty list to store the indices\n","    neighborhood_indices = []\n","    neighborhood_indices_dict= {}\n","\n","    if not len(small_df):\n","      neighborhood_indices, neighborhood_indices_dict\n","\n","    if verbose:\n","        print(f\"[FIND HITS] - Annotations to find:\")\n","        print(small_df)\n","\n","    # Iterate through each row in the smaller DataFrame\n","    for i, row in small_df.iterrows():\n","        if verbose:\n","          print(f\"Annotation with index {i}:\")\n","          print(row)\n","\n","        neighborhood_indices_dict[i] = []\n","        # Create conditions to check for vicinity in all three axes\n","        a_x = large_df[\"x\"] \u003e= (row[\"x\"] - threshold)\n","        b_x = large_df[\"x\"] \u003c= (row[\"x\"] + threshold)\n","        a_y = large_df[\"y\"] \u003e= (row[\"y\"] - threshold)\n","        b_y = large_df[\"y\"] \u003c= (row[\"y\"] + threshold)\n","        a_z = large_df[\"z\"] \u003e= (row[\"z\"] - threshold)\n","        b_z = large_df[\"z\"] \u003c= (row[\"z\"] + threshold)\n","\n","        # Combine conditions for all three axes\n","        vicinity_condition = a_x \u0026 b_x \u0026 a_y \u0026 b_y \u0026 a_z \u0026 b_z\n","\n","        # Find indices where all conditions are satisfied\n","        close_indices = large_df.index[vicinity_condition].tolist()\n","\n","        # Append these indices to the neighborhood_indices list\n","        neighborhood_indices.extend(close_indices)\n","        neighborhood_indices_dict[i] = (close_indices)\n","\n","    # Return unique indices as a numpy array\n","    return np.unique(neighborhood_indices), neighborhood_indices_dict\n","\n","\n","def sensitivity_score(candidates_df, annotations_df, threshold=15, verbose=False):\n","    \"\"\"\n","    Calculate the sensitivity score based on candidate detections and annotated coordinates.\n","\n","    Parameters:\n","    candidates_df (pd.DataFrame): DataFrame containing candidate coordinates with 'x', 'y', 'z' columns.\n","    annotations_df (pd.DataFrame): DataFrame containing annotated coordinates in the format with 'x', 'y', 'z' columns.\n","\n","    Returns:\n","    float: Sensitivity score, calculated as the ratio of correctly identified annotations to the total number of annotations.\n","    \"\"\"\n","    if not len(annotations_df):\n","      if verbose:\n","        print(f\"[SENSITIVITY] - No annotations for case -\u003e no sensitivity needed\")\n","      return 0\n","\n","    # Find neighborhood indices within a threshold distance\n","    candidate_indices, candidate_dict = find_neighborhood_indices(candidates_df, annotations_df, threshold)\n","\n","    # Initialize a score counter\n","    score = 0\n","\n","    # Iterate over the candidate dictionary to calculate the score\n","    for key, val in candidate_dict.items():\n","        if len(candidate_dict[key]): # If there is a candidate in the vicinity for the annotation, it counts as a 'hit'\n","            score += 1\n","\n","    # Return the sensitivity score\n","    return score / len(annotations_df)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1718044913922,"user":{"displayName":"Áron B. Gimesi","userId":"03233526977624235264"},"user_tz":-120},"id":"Ryb91CPTqZrZ","outputId":"f75a77f0-20db-4170-b74f-9fd4cae3e200"},"outputs":[{"name":"stdout","output_type":"stream","text":["[ANNOTATIONS] - simplified\n","Empty DataFrame\n","Columns: [x, y, z]\n","Index: []\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"convert_annotation_df(annotations_by_uid(\\\"1\",\n  \"rows\": 0,\n  \"fields\": [\n    {\n      \"column\": \"x\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"y\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"z\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe"},"text/html":["\n","  \u003cdiv id=\"df-41efb5a5-83e5-45e1-8842-b5b47d2ef762\" class=\"colab-df-container\"\u003e\n","    \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003ex\u003c/th\u003e\n","      \u003cth\u003ey\u003c/th\u003e\n","      \u003cth\u003ez\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e\n","    \u003cdiv class=\"colab-df-buttons\"\u003e\n","\n","  \u003cdiv class=\"colab-df-container\"\u003e\n","    \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-41efb5a5-83e5-45e1-8842-b5b47d2ef762')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\"\u003e\n","\n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\"\u003e\n","    \u003cpath d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/\u003e\n","  \u003c/svg\u003e\n","    \u003c/button\u003e\n","\n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","    \u003cscript\u003e\n","      const buttonEl =\n","        document.querySelector('#df-41efb5a5-83e5-45e1-8842-b5b47d2ef762 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-41efb5a5-83e5-45e1-8842-b5b47d2ef762');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    \u003c/script\u003e\n","  \u003c/div\u003e\n","\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n"],"text/plain":["Empty DataFrame\n","Columns: [x, y, z]\n","Index: []"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["convert_annotation_df(annotations_by_uid(\"1.3.6.1.4.1.14519.5.2.1.6279.6001.187451715205085403623595258748\", ANNOTATIONS_DF), verbose=True)"]},{"cell_type":"markdown","metadata":{"id":"1Ib48XdBIPzf"},"source":["# RUN FOR ONE"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1718044958583,"user":{"displayName":"Áron B. Gimesi","userId":"03233526977624235264"},"user_tz":-120},"id":"VjwbUORy5mz8"},"outputs":[],"source":["ONE_EXAMPLE_TEST = True"]},{"cell_type":"markdown","metadata":{"id":"scK1AV4921o9"},"source":["## LOAD EXAMPLE"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1718044958583,"user":{"displayName":"Áron B. Gimesi","userId":"03233526977624235264"},"user_tz":-120},"id":"DZ807lA_24N6"},"outputs":[],"source":["uid = \"1.3.6.1.4.1.14519.5.2.1.6279.6001.187451715205085403623595258748\" # analyzed case uid"]},{"cell_type":"markdown","metadata":{"id":"cWacjy4sO3uZ"},"source":["## CANDIDATES FOR UID"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1718044958584,"user":{"displayName":"Áron B. Gimesi","userId":"03233526977624235264"},"user_tz":-120},"id":"1SxYGjDWOZuE"},"outputs":[],"source":["def get_candidates_for_uid(uid, subsets_path, img_3d=None, annotations_df=None, verbose=False, debug=False):\n","  subset = None\n","  for subset_dir in os.listdir(subsets_path):\n","    filenames = os.listdir(os.path.join(subsets_path, subset_dir)) # filenames in subset directory\n","    filenames = list(map(lambda x: os.path.splitext(x)[0], filenames)) # filename w/o extension\n","    filenames = set(filenames) # unique filenames (because .mhd and .raw have same filename)\n","    if uid in filenames:\n","      subset = subset_dir\n","      break # if found -\u003e stop\n","\n","  if subset is None:\n","    raise FileNotFoundError(\"No image under this uid\")\n","\n","  if img_3d is None:\n","    img_path = os.path.join(subsets_path, subset, f\"{uid}.mhd\") # .mhd path\n","    mhd_img = sitk.ReadImage(img_path) # SimpleITK object\n","    img_3d = sitk.GetArrayFromImage(mhd_img)\n","    img_3d[img_3d \u003c AIR_TH] = AIR_TH\n","\n","  candidates_df, candidate_masks = process_slice_candidates(img_3d, verbose=verbose, debug=debug)\n","\n","  return candidates_df"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":273274,"status":"ok","timestamp":1718045231851,"user":{"displayName":"Áron B. Gimesi","userId":"03233526977624235264"},"user_tz":-120},"id":"6W2I9aYJoD3H","outputId":"03bc630d-d7c7-4c3e-81e9-6a0c7bba4de1"},"outputs":[{"name":"stdout","output_type":"stream","text":["[START] - processing slice #0\n","[DONE] - 0 candidates found for slice #0\n","\n","[STATUS] - 0 candidates found\n","[STATUS] - 125 slices left\n","\n","[START] - processing slice #1\n","[DONE] - 0 candidates found for slice #1\n","\n","[STATUS] - 0 candidates found\n","[STATUS] - 124 slices left\n","\n","[START] - processing slice #2\n","[DONE] - 0 candidates found for slice #2\n","\n","[STATUS] - 0 candidates found\n","[STATUS] - 123 slices left\n","\n","[START] - processing slice #3\n","[DONE] - 0 candidates found for slice #3\n","\n","[STATUS] - 0 candidates found\n","[STATUS] - 122 slices left\n","\n","[START] - processing slice #4\n","[DONE] - 0 candidates found for slice #4\n","\n","[STATUS] - 0 candidates found\n","[STATUS] - 121 slices left\n","\n","[START] - processing slice #5\n","[DONE] - 0 candidates found for slice #5\n","\n","[STATUS] - 0 candidates found\n","[STATUS] - 120 slices left\n","\n","[START] - processing slice #6\n","[DONE] - 0 candidates found for slice #6\n","\n","[STATUS] - 0 candidates found\n","[STATUS] - 119 slices left\n","\n","[START] - processing slice #7\n","[DONE] - 0 candidates found for slice #7\n","\n","[STATUS] - 0 candidates found\n","[STATUS] - 118 slices left\n","\n","[START] - processing slice #8\n","[DONE] - 0 candidates found for slice #8\n","\n","[STATUS] - 0 candidates found\n","[STATUS] - 117 slices left\n","\n","[START] - processing slice #9\n","[DONE] - 0 candidates found for slice #9\n","\n","[STATUS] - 0 candidates found\n","[STATUS] - 116 slices left\n","\n","[START] - processing slice #10\n","[DONE] - 1 candidates found for slice #10\n","\n","[STATUS] - 1 candidates found\n","[STATUS] - 115 slices left\n","\n","[START] - processing slice #11\n","[DONE] - 1 candidates found for slice #11\n","\n","[STATUS] - 2 candidates found\n","[STATUS] - 114 slices left\n","\n","[START] - processing slice #12\n","[DONE] - 2 candidates found for slice #12\n","\n","[STATUS] - 4 candidates found\n","[STATUS] - 113 slices left\n","\n","[START] - processing slice #13\n","[DONE] - 1 candidates found for slice #13\n","\n","[STATUS] - 5 candidates found\n","[STATUS] - 112 slices left\n","\n","[START] - processing slice #14\n","[DONE] - 0 candidates found for slice #14\n","\n","[STATUS] - 5 candidates found\n","[STATUS] - 111 slices left\n","\n","[START] - processing slice #15\n","[DONE] - 1 candidates found for slice #15\n","\n","[STATUS] - 6 candidates found\n","[STATUS] - 110 slices left\n","\n","[START] - processing slice #16\n","[DONE] - 1 candidates found for slice #16\n","\n","[STATUS] - 7 candidates found\n","[STATUS] - 109 slices left\n","\n","[START] - processing slice #17\n","[DONE] - 3 candidates found for slice #17\n","\n","[STATUS] - 10 candidates found\n","[STATUS] - 108 slices left\n","\n","[START] - processing slice #18\n","[DONE] - 1 candidates found for slice #18\n","\n","[STATUS] - 11 candidates found\n","[STATUS] - 107 slices left\n","\n","[START] - processing slice #19\n","[DONE] - 3 candidates found for slice #19\n","\n","[STATUS] - 14 candidates found\n","[STATUS] - 106 slices left\n","\n","[START] - processing slice #20\n","[DONE] - 7 candidates found for slice #20\n","\n","[STATUS] - 21 candidates found\n","[STATUS] - 105 slices left\n","\n","[START] - processing slice #21\n","[DONE] - 10 candidates found for slice #21\n","\n","[STATUS] - 31 candidates found\n","[STATUS] - 104 slices left\n","\n","[START] - processing slice #22\n","[DONE] - 12 candidates found for slice #22\n","\n","[STATUS] - 43 candidates found\n","[STATUS] - 103 slices left\n","\n","[START] - processing slice #23\n","[DONE] - 10 candidates found for slice #23\n","\n","[STATUS] - 53 candidates found\n","[STATUS] - 102 slices left\n","\n","[START] - processing slice #24\n","[DONE] - 11 candidates found for slice #24\n","\n","[STATUS] - 64 candidates found\n","[STATUS] - 101 slices left\n","\n","[START] - processing slice #25\n","[DONE] - 13 candidates found for slice #25\n","\n","[STATUS] - 77 candidates found\n","[STATUS] - 100 slices left\n","\n","[START] - processing slice #26\n","[DONE] - 7 candidates found for slice #26\n","\n","[STATUS] - 84 candidates found\n","[STATUS] - 99 slices left\n","\n","[START] - processing slice #27\n","[DONE] - 15 candidates found for slice #27\n","\n","[STATUS] - 99 candidates found\n","[STATUS] - 98 slices left\n","\n","[START] - processing slice #28\n","[DONE] - 13 candidates found for slice #28\n","\n","[STATUS] - 112 candidates found\n","[STATUS] - 97 slices left\n","\n","[START] - processing slice #29\n","[DONE] - 14 candidates found for slice #29\n","\n","[STATUS] - 126 candidates found\n","[STATUS] - 96 slices left\n","\n","[START] - processing slice #30\n","[DONE] - 11 candidates found for slice #30\n","\n","[STATUS] - 137 candidates found\n","[STATUS] - 95 slices left\n","\n","[START] - processing slice #31\n","[DONE] - 18 candidates found for slice #31\n","\n","[STATUS] - 155 candidates found\n","[STATUS] - 94 slices left\n","\n","[START] - processing slice #32\n","[DONE] - 12 candidates found for slice #32\n","\n","[STATUS] - 167 candidates found\n","[STATUS] - 93 slices left\n","\n","[START] - processing slice #33\n","[DONE] - 11 candidates found for slice #33\n","\n","[STATUS] - 178 candidates found\n","[STATUS] - 92 slices left\n","\n","[START] - processing slice #34\n","[DONE] - 9 candidates found for slice #34\n","\n","[STATUS] - 187 candidates found\n","[STATUS] - 91 slices left\n","\n","[START] - processing slice #35\n","[DONE] - 14 candidates found for slice #35\n","\n","[STATUS] - 201 candidates found\n","[STATUS] - 90 slices left\n","\n","[START] - processing slice #36\n","[DONE] - 17 candidates found for slice #36\n","\n","[STATUS] - 218 candidates found\n","[STATUS] - 89 slices left\n","\n","[START] - processing slice #37\n","[DONE] - 17 candidates found for slice #37\n","\n","[STATUS] - 235 candidates found\n","[STATUS] - 88 slices left\n","\n","[START] - processing slice #38\n","[DONE] - 19 candidates found for slice #38\n","\n","[STATUS] - 254 candidates found\n","[STATUS] - 87 slices left\n","\n","[START] - processing slice #39\n","[DONE] - 17 candidates found for slice #39\n","\n","[STATUS] - 271 candidates found\n","[STATUS] - 86 slices left\n","\n","[START] - processing slice #40\n","[DONE] - 20 candidates found for slice #40\n","\n","[STATUS] - 291 candidates found\n","[STATUS] - 85 slices left\n","\n","[START] - processing slice #41\n","[DONE] - 23 candidates found for slice #41\n","\n","[STATUS] - 314 candidates found\n","[STATUS] - 84 slices left\n","\n","[START] - processing slice #42\n","[DONE] - 27 candidates found for slice #42\n","\n","[STATUS] - 341 candidates found\n","[STATUS] - 83 slices left\n","\n","[START] - processing slice #43\n","[DONE] - 16 candidates found for slice #43\n","\n","[STATUS] - 357 candidates found\n","[STATUS] - 82 slices left\n","\n","[START] - processing slice #44\n","[DONE] - 15 candidates found for slice #44\n","\n","[STATUS] - 372 candidates found\n","[STATUS] - 81 slices left\n","\n","[START] - processing slice #45\n","[DONE] - 16 candidates found for slice #45\n","\n","[STATUS] - 388 candidates found\n","[STATUS] - 80 slices left\n","\n","[START] - processing slice #46\n","[DONE] - 19 candidates found for slice #46\n","\n","[STATUS] - 407 candidates found\n","[STATUS] - 79 slices left\n","\n","[START] - processing slice #47\n","[DONE] - 18 candidates found for slice #47\n","\n","[STATUS] - 425 candidates found\n","[STATUS] - 78 slices left\n","\n","[START] - processing slice #48\n","[DONE] - 31 candidates found for slice #48\n","\n","[STATUS] - 456 candidates found\n","[STATUS] - 77 slices left\n","\n","[START] - processing slice #49\n","[DONE] - 21 candidates found for slice #49\n","\n","[STATUS] - 477 candidates found\n","[STATUS] - 76 slices left\n","\n","[START] - processing slice #50\n","[DONE] - 25 candidates found for slice #50\n","\n","[STATUS] - 502 candidates found\n","[STATUS] - 75 slices left\n","\n","[START] - processing slice #51\n","[DONE] - 31 candidates found for slice #51\n","\n","[STATUS] - 533 candidates found\n","[STATUS] - 74 slices left\n","\n","[START] - processing slice #52\n","[DONE] - 20 candidates found for slice #52\n","\n","[STATUS] - 553 candidates found\n","[STATUS] - 73 slices left\n","\n","[START] - processing slice #53\n","[DONE] - 24 candidates found for slice #53\n","\n","[STATUS] - 577 candidates found\n","[STATUS] - 72 slices left\n","\n","[START] - processing slice #54\n","[DONE] - 27 candidates found for slice #54\n","\n","[STATUS] - 604 candidates found\n","[STATUS] - 71 slices left\n","\n","[START] - processing slice #55\n","[DONE] - 25 candidates found for slice #55\n","\n","[STATUS] - 629 candidates found\n","[STATUS] - 70 slices left\n","\n","[START] - processing slice #56\n","[DONE] - 28 candidates found for slice #56\n","\n","[STATUS] - 657 candidates found\n","[STATUS] - 69 slices left\n","\n","[START] - processing slice #57\n","[DONE] - 26 candidates found for slice #57\n","\n","[STATUS] - 683 candidates found\n","[STATUS] - 68 slices left\n","\n","[START] - processing slice #58\n","[DONE] - 23 candidates found for slice #58\n","\n","[STATUS] - 706 candidates found\n","[STATUS] - 67 slices left\n","\n","[START] - processing slice #59\n","[DONE] - 23 candidates found for slice #59\n","\n","[STATUS] - 729 candidates found\n","[STATUS] - 66 slices left\n","\n","[START] - processing slice #60\n","[DONE] - 25 candidates found for slice #60\n","\n","[STATUS] - 754 candidates found\n","[STATUS] - 65 slices left\n","\n","[START] - processing slice #61\n","[DONE] - 21 candidates found for slice #61\n","\n","[STATUS] - 775 candidates found\n","[STATUS] - 64 slices left\n","\n","[START] - processing slice #62\n","[DONE] - 20 candidates found for slice #62\n","\n","[STATUS] - 795 candidates found\n","[STATUS] - 63 slices left\n","\n","[START] - processing slice #63\n","[DONE] - 25 candidates found for slice #63\n","\n","[STATUS] - 820 candidates found\n","[STATUS] - 62 slices left\n","\n","[START] - processing slice #64\n","[DONE] - 29 candidates found for slice #64\n","\n","[STATUS] - 849 candidates found\n","[STATUS] - 61 slices left\n","\n","[START] - processing slice #65\n","[DONE] - 30 candidates found for slice #65\n","\n","[STATUS] - 879 candidates found\n","[STATUS] - 60 slices left\n","\n","[START] - processing slice #66\n","[DONE] - 29 candidates found for slice #66\n","\n","[STATUS] - 908 candidates found\n","[STATUS] - 59 slices left\n","\n","[START] - processing slice #67\n","[DONE] - 24 candidates found for slice #67\n","\n","[STATUS] - 932 candidates found\n","[STATUS] - 58 slices left\n","\n","[START] - processing slice #68\n","[DONE] - 22 candidates found for slice #68\n","\n","[STATUS] - 954 candidates found\n","[STATUS] - 57 slices left\n","\n","[START] - processing slice #69\n","[DONE] - 25 candidates found for slice #69\n","\n","[STATUS] - 979 candidates found\n","[STATUS] - 56 slices left\n","\n","[START] - processing slice #70\n","[DONE] - 28 candidates found for slice #70\n","\n","[STATUS] - 1007 candidates found\n","[STATUS] - 55 slices left\n","\n","[START] - processing slice #71\n","[DONE] - 25 candidates found for slice #71\n","\n","[STATUS] - 1032 candidates found\n","[STATUS] - 54 slices left\n","\n","[START] - processing slice #72\n","[DONE] - 25 candidates found for slice #72\n","\n","[STATUS] - 1057 candidates found\n","[STATUS] - 53 slices left\n","\n","[START] - processing slice #73\n","[DONE] - 19 candidates found for slice #73\n","\n","[STATUS] - 1076 candidates found\n","[STATUS] - 52 slices left\n","\n","[START] - processing slice #74\n","[DONE] - 20 candidates found for slice #74\n","\n","[STATUS] - 1096 candidates found\n","[STATUS] - 51 slices left\n","\n","[START] - processing slice #75\n","[DONE] - 23 candidates found for slice #75\n","\n","[STATUS] - 1119 candidates found\n","[STATUS] - 50 slices left\n","\n","[START] - processing slice #76\n","[DONE] - 22 candidates found for slice #76\n","\n","[STATUS] - 1141 candidates found\n","[STATUS] - 49 slices left\n","\n","[START] - processing slice #77\n","[DONE] - 21 candidates found for slice #77\n","\n","[STATUS] - 1162 candidates found\n","[STATUS] - 48 slices left\n","\n","[START] - processing slice #78\n","[DONE] - 13 candidates found for slice #78\n","\n","[STATUS] - 1175 candidates found\n","[STATUS] - 47 slices left\n","\n","[START] - processing slice #79\n","[DONE] - 15 candidates found for slice #79\n","\n","[STATUS] - 1190 candidates found\n","[STATUS] - 46 slices left\n","\n","[START] - processing slice #80\n","[DONE] - 15 candidates found for slice #80\n","\n","[STATUS] - 1205 candidates found\n","[STATUS] - 45 slices left\n","\n","[START] - processing slice #81\n","[DONE] - 17 candidates found for slice #81\n","\n","[STATUS] - 1222 candidates found\n","[STATUS] - 44 slices left\n","\n","[START] - processing slice #82\n","[DONE] - 16 candidates found for slice #82\n","\n","[STATUS] - 1238 candidates found\n","[STATUS] - 43 slices left\n","\n","[START] - processing slice #83\n","[DONE] - 9 candidates found for slice #83\n","\n","[STATUS] - 1247 candidates found\n","[STATUS] - 42 slices left\n","\n","[START] - processing slice #84\n","[DONE] - 11 candidates found for slice #84\n","\n","[STATUS] - 1258 candidates found\n","[STATUS] - 41 slices left\n","\n","[START] - processing slice #85\n","[DONE] - 10 candidates found for slice #85\n","\n","[STATUS] - 1268 candidates found\n","[STATUS] - 40 slices left\n","\n","[START] - processing slice #86\n","[DONE] - 8 candidates found for slice #86\n","\n","[STATUS] - 1276 candidates found\n","[STATUS] - 39 slices left\n","\n","[START] - processing slice #87\n","[DONE] - 6 candidates found for slice #87\n","\n","[STATUS] - 1282 candidates found\n","[STATUS] - 38 slices left\n","\n","[START] - processing slice #88\n","[DONE] - 6 candidates found for slice #88\n","\n","[STATUS] - 1288 candidates found\n","[STATUS] - 37 slices left\n","\n","[START] - processing slice #89\n","[DONE] - 3 candidates found for slice #89\n","\n","[STATUS] - 1291 candidates found\n","[STATUS] - 36 slices left\n","\n","[START] - processing slice #90\n","[DONE] - 7 candidates found for slice #90\n","\n","[STATUS] - 1298 candidates found\n","[STATUS] - 35 slices left\n","\n","[START] - processing slice #91\n","[DONE] - 4 candidates found for slice #91\n","\n","[STATUS] - 1302 candidates found\n","[STATUS] - 34 slices left\n","\n","[START] - processing slice #92\n","[DONE] - 6 candidates found for slice #92\n","\n","[STATUS] - 1308 candidates found\n","[STATUS] - 33 slices left\n","\n","[START] - processing slice #93\n","[DONE] - 2 candidates found for slice #93\n","\n","[STATUS] - 1310 candidates found\n","[STATUS] - 32 slices left\n","\n","[START] - processing slice #94\n","[DONE] - 2 candidates found for slice #94\n","\n","[STATUS] - 1312 candidates found\n","[STATUS] - 31 slices left\n","\n","[START] - processing slice #95\n","[DONE] - 2 candidates found for slice #95\n","\n","[STATUS] - 1314 candidates found\n","[STATUS] - 30 slices left\n","\n","[START] - processing slice #96\n","[DONE] - 3 candidates found for slice #96\n","\n","[STATUS] - 1317 candidates found\n","[STATUS] - 29 slices left\n","\n","[START] - processing slice #97\n","[DONE] - 4 candidates found for slice #97\n","\n","[STATUS] - 1321 candidates found\n","[STATUS] - 28 slices left\n","\n","[START] - processing slice #98\n","[DONE] - 2 candidates found for slice #98\n","\n","[STATUS] - 1323 candidates found\n","[STATUS] - 27 slices left\n","\n","[START] - processing slice #99\n","[DONE] - 2 candidates found for slice #99\n","\n","[STATUS] - 1325 candidates found\n","[STATUS] - 26 slices left\n","\n","[START] - processing slice #100\n","[DONE] - 1 candidates found for slice #100\n","\n","[STATUS] - 1326 candidates found\n","[STATUS] - 25 slices left\n","\n","[START] - processing slice #101\n","[DONE] - 1 candidates found for slice #101\n","\n","[STATUS] - 1327 candidates found\n","[STATUS] - 24 slices left\n","\n","[START] - processing slice #102\n","[DONE] - 2 candidates found for slice #102\n","\n","[STATUS] - 1329 candidates found\n","[STATUS] - 23 slices left\n","\n","[START] - processing slice #103\n","[DONE] - 1 candidates found for slice #103\n","\n","[STATUS] - 1330 candidates found\n","[STATUS] - 22 slices left\n","\n","[START] - processing slice #104\n","[DONE] - 2 candidates found for slice #104\n","\n","[STATUS] - 1332 candidates found\n","[STATUS] - 21 slices left\n","\n","[START] - processing slice #105\n","[DONE] - 2 candidates found for slice #105\n","\n","[STATUS] - 1334 candidates found\n","[STATUS] - 20 slices left\n","\n","[START] - processing slice #106\n","[DONE] - 3 candidates found for slice #106\n","\n","[STATUS] - 1337 candidates found\n","[STATUS] - 19 slices left\n","\n","[START] - processing slice #107\n","[DONE] - 1 candidates found for slice #107\n","\n","[STATUS] - 1338 candidates found\n","[STATUS] - 18 slices left\n","\n","[START] - processing slice #108\n","[DONE] - 1 candidates found for slice #108\n","\n","[STATUS] - 1339 candidates found\n","[STATUS] - 17 slices left\n","\n","[START] - processing slice #109\n","[DONE] - 0 candidates found for slice #109\n","\n","[STATUS] - 1339 candidates found\n","[STATUS] - 16 slices left\n","\n","[START] - processing slice #110\n","[DONE] - 2 candidates found for slice #110\n","\n","[STATUS] - 1341 candidates found\n","[STATUS] - 15 slices left\n","\n","[START] - processing slice #111\n","[DONE] - 1 candidates found for slice #111\n","\n","[STATUS] - 1342 candidates found\n","[STATUS] - 14 slices left\n","\n","[START] - processing slice #112\n","[DONE] - 0 candidates found for slice #112\n","\n","[STATUS] - 1342 candidates found\n","[STATUS] - 13 slices left\n","\n","[START] - processing slice #113\n","[DONE] - 0 candidates found for slice #113\n","\n","[STATUS] - 1342 candidates found\n","[STATUS] - 12 slices left\n","\n","[START] - processing slice #114\n","[DONE] - 0 candidates found for slice #114\n","\n","[STATUS] - 1342 candidates found\n","[STATUS] - 11 slices left\n","\n","[START] - processing slice #115\n","[DONE] - 0 candidates found for slice #115\n","\n","[STATUS] - 1342 candidates found\n","[STATUS] - 10 slices left\n","\n","[START] - processing slice #116\n","[DONE] - 0 candidates found for slice #116\n","\n","[STATUS] - 1342 candidates found\n","[STATUS] - 9 slices left\n","\n","[START] - processing slice #117\n","[DONE] - 0 candidates found for slice #117\n","\n","[STATUS] - 1342 candidates found\n","[STATUS] - 8 slices left\n","\n","[START] - processing slice #118\n","[DONE] - 0 candidates found for slice #118\n","\n","[STATUS] - 1342 candidates found\n","[STATUS] - 7 slices left\n","\n","[START] - processing slice #119\n","[DONE] - 0 candidates found for slice #119\n","\n","[STATUS] - 1342 candidates found\n","[STATUS] - 6 slices left\n","\n","[START] - processing slice #120\n","[DONE] - 0 candidates found for slice #120\n","\n","[STATUS] - 1342 candidates found\n","[STATUS] - 5 slices left\n","\n","[START] - processing slice #121\n","[DONE] - 0 candidates found for slice #121\n","\n","[STATUS] - 1342 candidates found\n","[STATUS] - 4 slices left\n","\n","[START] - processing slice #122\n","[DONE] - 0 candidates found for slice #122\n","\n","[STATUS] - 1342 candidates found\n","[STATUS] - 3 slices left\n","\n","[START] - processing slice #123\n","[DONE] - 0 candidates found for slice #123\n","\n","[STATUS] - 1342 candidates found\n","[STATUS] - 2 slices left\n","\n","[START] - processing slice #124\n","[DONE] - 0 candidates found for slice #124\n","\n","[STATUS] - 1342 candidates found\n","[STATUS] - 1 slices left\n","\n"]}],"source":["if ONE_EXAMPLE_TEST:\n","  candidates_df = get_candidates_for_uid(uid, SUBSETS_PATH, annotations_df=ANNOTATIONS_DF, verbose=True)"]},{"cell_type":"markdown","metadata":{"id":"PYPEwT69Yz-x"},"source":["## EXTRACT FEATURES FOR UID"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":53,"status":"ok","timestamp":1718045231852,"user":{"displayName":"Áron B. Gimesi","userId":"03233526977624235264"},"user_tz":-120},"id":"9QDefdE0WSm8"},"outputs":[],"source":["def extract_features_for_uid(uid, subsets_path, candidates_df, img_3d=None, verbose=False, debug=False):\n","  subset = None\n","  for subset_dir in os.listdir(subsets_path):\n","    filenames = os.listdir(os.path.join(subsets_path, subset_dir)) # filenames in subset directory\n","    filenames = list(map(lambda x: os.path.splitext(x)[0], filenames)) # filename w/o extension\n","    filenames = set(filenames) # unique filenames (because .mhd and .raw have same filename)\n","    if uid in filenames:\n","      subset = subset_dir\n","      break # if found -\u003e stop\n","\n","  if subset is None:\n","    raise FileNotFoundError(\"No image under this uid\")\n","\n","  if img_3d is None:\n","    img_path = os.path.join(subsets_path, subset, f\"{uid}.mhd\") # .mhd path\n","    mhd_img = sitk.ReadImage(img_path) # SimpleITK object\n","\n","    img_3d = sitk.GetArrayFromImage(mhd_img)\n","    img_3d[img_3d \u003c AIR_TH] = AIR_TH\n","\n","  mask_3d = binarize_lung_3d(img_3d).astype(bool)\n","\n","\n","  # EXTRACT FEATURES\n","  candidate_features_df = pd.DataFrame()\n","\n","  for i, row in candidates_df.iterrows():\n","    if verbose:\n","      print(f\"[FEATURES] - {uid} - extracting features for candidate #{i}\")\n","\n","    slice_ = img_3d[row[\"z\"],:,:] # get slice for candidate\n","    candidate_roi, _ = create_patch(slice_, (row[\"y\"],row[\"x\"]))\n","\n","    slice_mask = mask_3d[row[\"z\"],:,:] # get lung mask for slice\n","    candidate_roi_mask, _ = create_patch(slice_mask, (row[\"y\"],row[\"x\"]))\n","    if debug:\n","      debugger(candidate_roi, \"candidate_roi\")\n","\n","    try:\n","      fts = extract_features(candidate_roi, candidate_roi_mask, debug=debug)\n","      if verbose:\n","        print(f\"Features:\")\n","        print(fts)\n","      features_df = pd.DataFrame([fts], index=[i])\n","      candidate_features_df = pd.concat([candidate_features_df, features_df])\n","    except Exception as e: # If feature extraction fails (likely empty image) remove the candidate\n","      if verbose:\n","        print(f\"[ERROR] - {e}\")\n","        print(f\"[INFO] - dropping candidate #{i}\")\n","      candidates_df = candidates_df.drop(index=i)\n","      continue\n","\n","\n","  return candidate_features_df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"14iyMLYwIs9W5HgfUoAjy7z7T4D66C2RL"},"id":"KepAZAiBXJAr","outputId":"3036e1ba-0a6f-43c6-ba1b-f60d870b4dca"},"outputs":[],"source":["if ONE_EXAMPLE_TEST:\n","  features_df = extract_features_for_uid(uid, SUBSETS_PATH, candidates_df, verbose=True, debug=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"HgPVw8mCVMMQ"},"outputs":[],"source":["if ONE_EXAMPLE_TEST:\n","  features_df"]},{"cell_type":"markdown","metadata":{"id":"WCAFxxF3zAka"},"source":["## DISCARD CANDIDATES WITHOUT RELEVANT INFORMATION"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"WG5yaN2fxrT6"},"outputs":[],"source":["if ONE_EXAMPLE_TEST:\n","  intersection = np.intersect1d(candidates_df.index, features_df.index)\n","\n","  filtered_candidates_df = candidates_df.loc[intersection]\n","\n","  filtered_candidates_df = pd.concat([filtered_candidates_df, features_df], axis=1)\n","  filtered_candidates_df[\"Class\"] = np.zeros_like(filtered_candidates_df.index)\n","  filtered_candidates_df"]},{"cell_type":"markdown","metadata":{"id":"5rlHF667ynH6"},"source":["## FIND TRUE POSITIVE CANDIDATES"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ISE24fwyMs1A"},"outputs":[{"name":"stdout","output_type":"stream","text":["[FIND HITS] - Annotations to find:\n","     x    y    z\n","8  354  279  121\n","Annotation with index 8:\n","x    354\n","y    279\n","z    121\n","Name: 8, dtype: int64\n"]},{"ename":"KeyError","evalue":"\"None of [Index([926, 927, 931, 934, 936, 938, 940, 942], dtype='int64')] are in the [index]\"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-53-a7f4e0601fc4\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 1\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mhit_indexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhit_indexes_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_neighborhood_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_annotation_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotations_by_uid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mANNOTATIONS_DF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighborhood_th\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 5\u001b[0;31m   \u001b[0mfiltered_candidates_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhit_indexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Class\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhit_indexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Hit indexes count: {len(hit_indexes)} --- Positive class count: {len(filtered_candidates_df[filtered_candidates_df['Class'] == 1])} \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 845\u001b[0;31m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_setitem_indexer\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    708\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIndexingError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m                 \u001b[0;31m# suppress \"Too many indexers\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 710\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_tuple\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[0;31m# Note: we assume _tupleize_axis_indexer has been called, if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 927\u001b[0;31m         \u001b[0mkeyidx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyidx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m\u003clistcomp\u003e\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[0;31m# Note: we assume _tupleize_axis_indexer has been called, if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 927\u001b[0;31m         \u001b[0mkeyidx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyidx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1424\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1426\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1427\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1460\u001b[0m         \u001b[0maxis_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1462\u001b[0;31m         \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5875\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 5877\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5879\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5936\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muse_interval_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5937\u001b[0m                     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 5938\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5940\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"None of [Index([926, 927, 931, 934, 936, 938, 940, 942], dtype='int64')] are in the [index]\""]}],"source":["if ONE_EXAMPLE_TEST:\n","  neighborhood_th = 12\n","\n","  hit_indexes, hit_indexes_dict = find_neighborhood_indices(candidates_df, convert_annotation_df(annotations_by_uid(uid, ANNOTATIONS_DF)), neighborhood_th, verbose=True)\n","  filtered_candidates_df.loc[hit_indexes, \"Class\"] = np.full((len(hit_indexes)), 1)\n","\n","  print(f\"Hit indexes count: {len(hit_indexes)} --- Positive class count: {len(filtered_candidates_df[filtered_candidates_df['Class'] == 1])} \")\n","\n","  filtered_candidates_df[filtered_candidates_df[\"Class\"] == 1]\n","\n","  print(hit_indexes_dict)"]},{"cell_type":"markdown","metadata":{"id":"bQXahXH10Gdb"},"source":["## EXTRACT TO CSV TO TRAIN MODEL"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"FPICeYS20MFa"},"outputs":[],"source":["def get_trainable_candidate_features(uid, subsets_path, annotations_df, neighborhood_th = 12, save_csv_path=None, verbose=False, debug=False):\n","\n","  # SAFEGUARD NOT TO RUN UNNECESSARY OPERATIONS\n","  if not len(annotations_by_uid(uid, annotations_df)):\n","    raise Exception('NO ANNOTATIONS FOR THIS CASE')\n","\n","  subset = None\n","  for subset_dir in os.listdir(subsets_path):\n","    filenames = os.listdir(os.path.join(subsets_path, subset_dir)) # filenames in subset directory\n","    filenames = list(map(lambda x: os.path.splitext(x)[0], filenames)) # filename w/o extension\n","    filenames = set(filenames) # unique filenames (because .mhd and .raw have same filename)\n","    if uid in filenames:\n","      subset = subset_dir\n","      break # if found -\u003e stop\n","\n","  if subset is None:\n","    raise FileNotFoundError(\"No image under this uid\")\n","\n","  img_path = os.path.join(subsets_path, subset, f\"{uid}.mhd\") # .mhd path\n","  mhd_img = sitk.ReadImage(img_path) # SimpleITK object\n","  img_3d = sitk.GetArrayFromImage(mhd_img)\n","  img_3d[img_3d \u003c AIR_TH] = AIR_TH # Normalize minimum intensity value\n","\n","  print(f\"[GET CANDIDATES] - {uid}\")\n","  candidates_df = get_candidates_for_uid(uid, subsets_path, img_3d=img_3d, annotations_df=annotations_df, verbose=verbose)\n","  print(f\"[GET CANDIDATES] - {uid}  - {len(candidates_df)} candidates for case\")\n","\n","  print(f\"[GET CANDIDATE FEATURES] - {uid}\")\n","  features_df = extract_features_for_uid(uid, subsets_path, candidates_df, img_3d=img_3d, verbose=verbose, debug=debug)\n","  print(f\"[GET CANDIDATE FEATURES] - {uid}  - {len(features_df)} candidates for case\")\n","\n","  intersection = np.intersect1d(candidates_df.index, features_df.index)\n","\n","  filtered_candidates_df = candidates_df.loc[intersection]\n","\n","  filtered_candidates_df = pd.concat([filtered_candidates_df, features_df], axis=1)\n","  filtered_candidates_df[\"Class\"] = np.zeros_like(filtered_candidates_df.index)\n","\n","  simple_annotations_df = convert_annotation_df(annotations_by_uid(uid, annotations_df))\n","\n","  print(f\"[GET TRUE POSITIVE CANDIDATES] - {uid}\")\n","  hit_indexes, hit_indexes_dict = find_neighborhood_indices(filtered_candidates_df, simple_annotations_df, neighborhood_th, verbose=verbose)\n","  filtered_candidates_df.loc[hit_indexes, \"Class\"] = np.full((len(hit_indexes)), 1)\n","  print(f\"[GET TRUE POSITIVE CANDIDATES] - {uid}  - {len(hit_indexes)} TP candidates for case\")\n","\n","  sensitivity = sensitivity_score(filtered_candidates_df, simple_annotations_df, verbose=verbose)\n","  print(f\"[GET TRUE POSITIVE CANDIDATES] - {uid}  - {sensitivity} sensitivity for\")\n","\n","  if save_csv_path is not None:\n","    if verbose:\n","      print(f\"[SAVE] - {uid} - candidate features csv\")\n","    # CHANGE THIS LINE FOR FUTURE NOTEBOOKS WITH DIFFERENT PROPERTIES\n","    save_csv_path = filtered_candidates_df.to_csv(os.path.join(save_csv_path, subset, f\"{uid}_candidates.csv\"))\n","\n","  return filtered_candidates_df, (hit_indexes, hit_indexes_dict), sensitivity"]},{"cell_type":"markdown","metadata":{"id":"v8rhCw_esdS4"},"source":["## TEST"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"JF0nWmWtYcvN"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'1.3.6.1.4.1.14519.5.2.1.6279.6001.733642690503782454656013446707', '1.3.6.1.4.1.14519.5.2.1.6279.6001.292994770358625142596171316474', '1.3.6.1.4.1.14519.5.2.1.6279.6001.436403998650924660479049012235', '1.3.6.1.4.1.14519.5.2.1.6279.6001.265570697208310960298668720669', '1.3.6.1.4.1.14519.5.2.1.6279.6001.199261544234308780356714831537', '1.3.6.1.4.1.14519.5.2.1.6279.6001.170706757615202213033480003264', '1.3.6.1.4.1.14519.5.2.1.6279.6001.112767175295249119452142211437', '1.3.6.1.4.1.14519.5.2.1.6279.6001.212608679077007918190529579976', '1.3.6.1.4.1.14519.5.2.1.6279.6001.747803439040091794717626507402', '1.3.6.1.4.1.14519.5.2.1.6279.6001.139577698050713461261415990027', '1.3.6.1.4.1.14519.5.2.1.6279.6001.179683407589764683292800449011', '1.3.6.1.4.1.14519.5.2.1.6279.6001.855232435861303786204450738044', '1.3.6.1.4.1.14519.5.2.1.6279.6001.121805476976020513950614465787', '1.3.6.1.4.1.14519.5.2.1.6279.6001.306140003699110313373771452136', '1.3.6.1.4.1.14519.5.2.1.6279.6001.265780642925621389994857727416', '1.3.6.1.4.1.14519.5.2.1.6279.6001.215104063467523905369326175410', '1.3.6.1.4.1.14519.5.2.1.6279.6001.292576688635952269497781991202', '1.3.6.1.4.1.14519.5.2.1.6279.6001.330643702676971528301859647742', '1.3.6.1.4.1.14519.5.2.1.6279.6001.334166493392278943610545989413', '1.3.6.1.4.1.14519.5.2.1.6279.6001.311476128731958142981941696518', '1.3.6.1.4.1.14519.5.2.1.6279.6001.196251645377731223510086726530', '1.3.6.1.4.1.14519.5.2.1.6279.6001.237215747217294006286437405216', '1.3.6.1.4.1.14519.5.2.1.6279.6001.193721075067404532739943086458', '1.3.6.1.4.1.14519.5.2.1.6279.6001.712472578497712558367294720243', '1.3.6.1.4.1.14519.5.2.1.6279.6001.145510611155363050427743946446', '1.3.6.1.4.1.14519.5.2.1.6279.6001.340158437895922179455019686521', '1.3.6.1.4.1.14519.5.2.1.6279.6001.291156498203266896953765649282', '1.3.6.1.4.1.14519.5.2.1.6279.6001.771741891125176943862272696845', '1.3.6.1.4.1.14519.5.2.1.6279.6001.472487466001405705666001578363', '1.3.6.1.4.1.14519.5.2.1.6279.6001.300270516469599170290456821227', '1.3.6.1.4.1.14519.5.2.1.6279.6001.182192086929819295877506541021', '1.3.6.1.4.1.14519.5.2.1.6279.6001.230491296081537726468075344411', '1.3.6.1.4.1.14519.5.2.1.6279.6001.237915456403882324748189195892', '1.3.6.1.4.1.14519.5.2.1.6279.6001.194632613233275988184244485809', '1.3.6.1.4.1.14519.5.2.1.6279.6001.124822907934319930841506266464', '1.3.6.1.4.1.14519.5.2.1.6279.6001.229860476925100292554329427970', '1.3.6.1.4.1.14519.5.2.1.6279.6001.176869045992276345870480098568', '1.3.6.1.4.1.14519.5.2.1.6279.6001.193964947698259739624715468431', '1.3.6.1.4.1.14519.5.2.1.6279.6001.174449669706458092793093760291', '1.3.6.1.4.1.14519.5.2.1.6279.6001.109882169963817627559804568094', '1.3.6.1.4.1.14519.5.2.1.6279.6001.188385286346390202873004762827', '1.3.6.1.4.1.14519.5.2.1.6279.6001.765459236550358748053283544075', '1.3.6.1.4.1.14519.5.2.1.6279.6001.250481236093201801255751845296', '1.3.6.1.4.1.14519.5.2.1.6279.6001.148935306123327835217659769212', '1.3.6.1.4.1.14519.5.2.1.6279.6001.173931884906244951746140865701', '1.3.6.1.4.1.14519.5.2.1.6279.6001.121108220866971173712229588402', '1.3.6.1.4.1.14519.5.2.1.6279.6001.134519406153127654901640638633', '1.3.6.1.4.1.14519.5.2.1.6279.6001.387954549120924524005910602207', '1.3.6.1.4.1.14519.5.2.1.6279.6001.167661207884826429102690781600', '1.3.6.1.4.1.14519.5.2.1.6279.6001.268030488196493755113553009785', '1.3.6.1.4.1.14519.5.2.1.6279.6001.300693623747082239407271583452', '1.3.6.1.4.1.14519.5.2.1.6279.6001.328944769569002417592093467626', '1.3.6.1.4.1.14519.5.2.1.6279.6001.138813197521718693188313387015', '1.3.6.1.4.1.14519.5.2.1.6279.6001.927394449308471452920270961822', '1.3.6.1.4.1.14519.5.2.1.6279.6001.188619674701053082195613114069', '1.3.6.1.4.1.14519.5.2.1.6279.6001.440226700369921575481834344455', '1.3.6.1.4.1.14519.5.2.1.6279.6001.195557219224169985110295082004', '1.3.6.1.4.1.14519.5.2.1.6279.6001.259453428008507791234730686014', '1.3.6.1.4.1.14519.5.2.1.6279.6001.603126300703296693942875967838', '1.3.6.1.4.1.14519.5.2.1.6279.6001.222052723822248889877676736332', '1.3.6.1.4.1.14519.5.2.1.6279.6001.340012777775661021262977442176', '1.3.6.1.4.1.14519.5.2.1.6279.6001.229096941293122177107846044795', '1.3.6.1.4.1.14519.5.2.1.6279.6001.300392272203629213913702120739', '1.3.6.1.4.1.14519.5.2.1.6279.6001.227968442353440630355230778531', '1.3.6.1.4.1.14519.5.2.1.6279.6001.229960820686439513664996214638', '1.3.6.1.4.1.14519.5.2.1.6279.6001.141345499716190654505508410197', '1.3.6.1.4.1.14519.5.2.1.6279.6001.272344603176687884771013620823', '1.3.6.1.4.1.14519.5.2.1.6279.6001.150097650621090951325113116280', '1.3.6.1.4.1.14519.5.2.1.6279.6001.187694838527128312070807533473', '1.3.6.1.4.1.14519.5.2.1.6279.6001.337005960787660957389988207064', '1.3.6.1.4.1.14519.5.2.1.6279.6001.114914167428485563471327801935', '1.3.6.1.4.1.14519.5.2.1.6279.6001.286061375572911414226912429210', '1.3.6.1.4.1.14519.5.2.1.6279.6001.946129570505893110165820050204', '1.3.6.1.4.1.14519.5.2.1.6279.6001.124656777236468248920498636247', '1.3.6.1.4.1.14519.5.2.1.6279.6001.330043769832606379655473292782', '1.3.6.1.4.1.14519.5.2.1.6279.6001.217754016294471278921686508169', '1.3.6.1.4.1.14519.5.2.1.6279.6001.558286136379689377915919180358', '1.3.6.1.4.1.14519.5.2.1.6279.6001.153985109349433321657655488650', '1.3.6.1.4.1.14519.5.2.1.6279.6001.392861216720727557882279374324', '1.3.6.1.4.1.14519.5.2.1.6279.6001.138674679709964033277400089532', '1.3.6.1.4.1.14519.5.2.1.6279.6001.129650136453746261130135157590', '1.3.6.1.4.1.14519.5.2.1.6279.6001.309901913847714156367981722205', '1.3.6.1.4.1.14519.5.2.1.6279.6001.220596530836092324070084384692', '1.3.6.1.4.1.14519.5.2.1.6279.6001.102681962408431413578140925249', '1.3.6.1.4.1.14519.5.2.1.6279.6001.765930210026773090100532964804', '1.3.6.1.4.1.14519.5.2.1.6279.6001.215785045378334625097907422785', '1.3.6.1.4.1.14519.5.2.1.6279.6001.254929810944557499537650429296'}\n"]}],"source":["print(UIDS)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"PgUJuhsa0WV7"},"outputs":[{"ename":"Exception","evalue":"NO ANNOTATIONS FOR THIS CASE","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-40-ba7b507e7324\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 1\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0muid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"1.3.6.1.4.1.14519.5.2.1.6279.6001.511347030803753100045216493273\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 4\u001b[0;31m   candidate_features, hit_indexes_tuple, sensitivity = get_trainable_candidate_features(\n\u001b[0m\u001b[1;32m      5\u001b[0m       \u001b[0muid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0mSUBSETS_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m\u003cipython-input-38-044c1622a905\u003e\u001b[0m in \u001b[0;36mget_trainable_candidate_features\u001b[0;34m(uid, subsets_path, annotations_df, neighborhood_th, save_csv_path, verbose, debug)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m# SAFEGUARD NOT TO RUN UNNECESSARY OPERATIONS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotations_by_uid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 5\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NO ANNOTATIONS FOR THIS CASE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0msubset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mException\u001b[0m: NO ANNOTATIONS FOR THIS CASE"]}],"source":["if ONE_EXAMPLE_TEST:\n","  uid = \"1.3.6.1.4.1.14519.5.2.1.6279.6001.511347030803753100045216493273\"\n","\n","  candidate_features, hit_indexes_tuple, sensitivity = get_trainable_candidate_features(\n","      uid,\n","      SUBSETS_PATH,\n","      ANNOTATIONS_DF,\n","      neighborhood_th=12,\n","      save_csv_path=SAVE_FOLDER_PATH,\n","      verbose=False,\n","      debug=False\n","  )\n","\n","  print(f\"sensitivity: {sensitivity}\")\n","\n","  print(f\"annotations:\")\n","  print(convert_annotation_df(annotations_by_uid(uid, ANNOTATIONS_DF)))\n","\n","  print(f\"hit_indexes for annotation indexes: {hit_indexes_tuple[1]}\")\n","  print(f\"hit_indexes: {hit_indexes_tuple[0]}\")\n"]},{"cell_type":"markdown","metadata":{"id":"1MJeHEui9Okx"},"source":["# RUN FOR ALL CASES IN SUBSET"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1718044930356,"user":{"displayName":"Áron B. Gimesi","userId":"03233526977624235264"},"user_tz":-120},"id":"7n7lc72lWCjN"},"outputs":[],"source":["if ONE_EXAMPLE_TEST:\n","  neighborhood_th = 12\n","\n","  result_dict = {}\n","\n","  for i, uid in enumerate(UIDS):\n","    result_dict[uid] = {}\n","\n","    # If there is no annotation for a case skip it\n","    if not len(annotations_by_uid(uid, ANNOTATIONS_DF)):\n","      continue\n","\n","    # Check if feature extraction file already exists\n","    save_folder_subset_path = os.path.join(SAVE_FOLDER_PATH, SUBSET)\n","    save_filename =  f\"{uid}_candidates.csv\"\n","    features_file_path = os.path.join(save_folder_subset_path, save_filename)\n","    already_extracted = os.listdir(save_folder_subset_path)\n","\n","    # If feature_extraction file already exists skip it\n","    if save_filename in os.listdir(save_folder_subset_path):\n","      print(f\"[SKIP] - {uid}  - features have already been extracted for case\")\n","      print(\"_____________________________________\\n\")\n","\n","      continue\n","\n","    print(f\"[START] - {uid} - processing case\")\n","    candidate_features, hit_indexes, sensitivity = get_trainable_candidate_features(\n","        uid,\n","        SUBSETS_PATH,\n","        ANNOTATIONS_DF,\n","        neighborhood_th=neighborhood_th,\n","        save_csv_path=SAVE_FOLDER_PATH,\n","        verbose=False,\n","        debug=False\n","    )\n","\n","\n","    result_dict[uid][\"sensitivity\"] = sensitivity\n","    result_dict[uid][\"hit_indexes\"] = hit_indexes\n","\n","    print(f\"[DONE] - {uid} - processing case\")\n","    print(\"\\n_____________________________________\\n\")\n","\n","    # if read_in_count \u003e 5:\n","    #   break\n","\n"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"executionInfo":{"elapsed":390,"status":"error","timestamp":1718044932075,"user":{"displayName":"Áron B. Gimesi","userId":"03233526977624235264"},"user_tz":-120},"id":"2hQYWveh4H5b","outputId":"fe64dc6c-7cbf-4776-cb94-f60f245f543c"},"outputs":[{"ename":"NameError","evalue":"name 'result_dict' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-28-24701a844412\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 1\u003e\u001b[0;34m()\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0mresult_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'result_dict' is not defined"]}],"source":["result_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mpif3X7Bs9u-"},"outputs":[],"source":["raise Exception(\"PLEASE STOP HERE\")"]},{"cell_type":"markdown","metadata":{"id":"oLj7WojQQfke"},"source":["# ASYNC RUN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QVwDFIuSWEto"},"outputs":[],"source":["SUBSET"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aalK1WEH2pK3"},"outputs":[],"source":["import os\n","from concurrent.futures import ThreadPoolExecutor\n","\n","def process_uid(uid, annotations_df, subsets_path, save_folder_path, subset, neighborhood_th):\n","    result = {}\n","    # If there is no annotation for a case skip it\n","    if not len(annotations_by_uid(uid, annotations_df)):\n","        print(f\"[SKIP] - {uid} - no annotations for case\\n\")\n","        return uid, None\n","\n","    # Check if feature extraction file already exists\n","    save_folder_subset_path = os.path.join(save_folder_path, subset)\n","    save_filename = f\"{uid}_candidates.csv\"\n","    features_file_path = os.path.join(save_folder_subset_path, save_filename)\n","    already_extracted = os.listdir(save_folder_subset_path)\n","\n","    # If feature_extraction file already exists skip it\n","    if save_filename in already_extracted:\n","        print(f\"[SKIP] - {uid} - features have already been extracted for case\\n\")\n","        print(\"_____________________________________\\n\")\n","        return uid, None\n","\n","    print(f\"[START] - {uid}  - processing case {uid}\\n\")\n","    candidate_features, hit_indexes, sensitivity = get_trainable_candidate_features(\n","        uid,\n","        subsets_path,\n","        annotations_df,\n","        neighborhood_th=neighborhood_th,\n","        save_csv_path=save_folder_path,\n","        verbose=False,\n","        debug=False\n","    )\n","\n","    result[\"sensitivity\"] = sensitivity\n","    result[\"hit_indexes\"] = hit_indexes\n","\n","    print(f\"[DONE] - {uid} - processing case\")\n","    print(\"\\n_____________________________________\\n\")\n","\n","    return uid, result\n","\n","def extract_features_concurrently(uids, annotations_df, subsets_path, save_folder_path, subset, neighborhood_th, max_workers=3):\n","    result_dict = {}\n","\n","    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n","        futures = {executor.submit(process_uid, uid, annotations_df, subsets_path, save_folder_path, subset, neighborhood_th): uid for uid in uids}\n","\n","        for future in futures:\n","            uid, result = future.result()\n","            if result is not None:\n","                result_dict[uid] = result\n","\n","    return result_dict\n","\n","result_dict = extract_features_concurrently(UIDS, ANNOTATIONS_DF, SUBSETS_PATH, SAVE_FOLDER_PATH, SUBSET, neighborhood_th, max_workers=5)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pFDOV5UuZCkt"},"outputs":[],"source":[]}],"metadata":{"colab":{"machine_shape":"hm","name":"","toc_visible":true,"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}