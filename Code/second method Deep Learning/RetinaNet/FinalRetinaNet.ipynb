{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deVz-WcEOLBT",
        "outputId": "41a18ee1-670f-4570-cbde-dca3a2f013ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting SimpleITK\n",
            "  Downloading SimpleITK-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: SimpleITK\n",
            "Successfully installed SimpleITK-2.3.1\n"
          ]
        }
      ],
      "source": [
        "pip install SimpleITK"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Loading and Preprocessing\n",
        "\n",
        "\n",
        "Defining paths and loading the dataset.\n",
        "\n",
        "Getting lists of subset images files.\n",
        "\n",
        "Combining file lists and removing duplicates.\n",
        "\n",
        "Defining functions to create masks (RECTANGULAR PATCHES) and retrieve filenames.\n",
        "\n",
        "Loading annotations and mapping series UIDs to filenames.\n",
        "\n",
        "Constructing a DataFrame with image data, mask data, and class labels."
      ],
      "metadata": {
        "id": "uQweq8-E2f8l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqU1NfMfOWLn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.models.detection import retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from glob import glob\n",
        "import cv2\n",
        "import SimpleITK as sitk\n",
        "from google.colab import drive\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths and load the dataset\n",
        "file_path_0 = \"/content/drive/Shareddrives/IA DL_project/ML IA/LUNA16/subsets/subset0\"\n",
        "file_path_1 = \"/content/drive/Shareddrives/IA DL_project/ML IA/LUNA16/subsets/subset1\"\n",
        "annotations_path = \"/content/drive/Shareddrives/IA DL_project/ML IA/LUNA16/annotations.csv\"\n",
        "\n",
        "# Getting list of image files from both subsets\n",
        "file_list_0 = glob(file_path_0 + \"/*.mhd\")\n",
        "file_list_1 = glob(file_path_1 + \"/*.mhd\")\n",
        "\n",
        "# Combine file lists and remove duplicates\n",
        "file_list = list(set(file_list_0 + file_list_1))\n",
        "\n",
        "# List filenames in subset0 and subset1 directories to understand the format\n",
        "print(\"Sample filenames in subset0:\", [os.path.basename(f) for f in file_list_0][:5])\n",
        "print(\"Sample filenames in subset1:\", [os.path.basename(f) for f in file_list_1][:5])\n",
        "\n",
        "# Function to correctly extract series UID from filename\n",
        "def get_filename(file_list, case):\n",
        "    for f in file_list:\n",
        "        if case in os.path.basename(f):\n",
        "            return f\n",
        "    return None\n",
        "\n",
        "# Load annotations\n",
        "df_node = pd.read_csv(annotations_path)\n",
        "\n",
        "# Extract series UIDs from filenames\n",
        "series_uids_from_files = [os.path.basename(f).split('.')[0] for f in file_list]\n",
        "print(\"Sample series UIDs from filenames:\", series_uids_from_files[:5])\n",
        "\n",
        "# Verify series UID compatibility\n",
        "unique_seriesuid_annotations = df_node['seriesuid'].nunique()\n",
        "unique_seriesuid_images = len(set(series_uids_from_files))\n",
        "\n",
        "print(\"Unique series UID in annotations:\", unique_seriesuid_annotations)\n",
        "print(\"Unique series UID in image files:\", unique_seriesuid_images)\n",
        "\n",
        "# Map the series UID to file names\n",
        "df_node[\"file\"] = df_node[\"seriesuid\"].map(lambda file_name: get_filename(file_list, file_name))\n",
        "df_node = df_node.dropna(subset=[\"file\"])  # Drop rows where file mapping is not found\n",
        "\n",
        "print(\"Annotations with associated files:\", df_node.shape[0])\n",
        "print(df_node.head())\n",
        "\n",
        "# Define DataFrame columns\n",
        "columns = [\"seriesuid\", \"sliceindex\", \"imagedata\", \"bbox\", \"class\"]\n",
        "data = []\n",
        "\n",
        "# Define target size for downsampling\n",
        "target_size = (256, 256)\n",
        "\n",
        "# Function to convert 3D center and diameter to 2D bounding box\n",
        "def convert_to_2d_bounding_box(center, diam, spacing, origin):\n",
        "    v_center = (center[:2] - origin[:2]) / spacing[:2]  # Only X, Y coordinates\n",
        "    diam_pixels = diam / spacing[:2]\n",
        "    x_min = int(v_center[0] - diam_pixels[0] / 2)\n",
        "    x_max = int(v_center[0] + diam_pixels[0] / 2)\n",
        "    y_min = int(v_center[1] - diam_pixels[1] / 2)\n",
        "    y_max = int(v_center[1] + diam_pixels[1] / 2)\n",
        "    return [x_min, y_min, x_max, y_max]\n",
        "\n",
        "# Function to load images and determine if they contain nodules\n",
        "def load_images_with_nodules(file_list, annotations_df, target_size=(256, 256)):\n",
        "    data = []\n",
        "    for img_file in tqdm(file_list):\n",
        "        mini_df = annotations_df[annotations_df[\"file\"] == img_file]\n",
        "        itk_img = sitk.ReadImage(img_file)\n",
        "        img_array = sitk.GetArrayFromImage(itk_img)\n",
        "        origin = np.array(itk_img.GetOrigin())\n",
        "        spacing = np.array(itk_img.GetSpacing())\n",
        "        original_size = img_array.shape[1:3]\n",
        "\n",
        "        if mini_df.shape[0] > 0:\n",
        "            for _, row in mini_df.iterrows():\n",
        "                node_x, node_y, node_z = row[\"coordX\"], row[\"coordY\"], row[\"coordZ\"]\n",
        "                diam = row[\"diameter_mm\"]\n",
        "                center = np.array([node_x, node_y, node_z])\n",
        "                i_z = int(np.rint((node_z - origin[2]) / spacing[2]))\n",
        "\n",
        "                if 0 <= i_z < img_array.shape[0]:  # Check if the z-index is within the slice range\n",
        "                    bbox = convert_to_2d_bounding_box(center, diam, spacing, origin)\n",
        "                    img_resized = cv2.resize(img_array[i_z], target_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "                    # Scale bounding box coordinates to match resized image\n",
        "                    scale_x = target_size[0] / original_size[1]\n",
        "                    scale_y = target_size[1] / original_size[0]\n",
        "                    bbox_resized = [int(bbox[0] * scale_x), int(bbox[1] * scale_y), int(bbox[2] * scale_x), int(bbox[3] * scale_y)]\n",
        "\n",
        "                    data.append([row[\"seriesuid\"], i_z, img_resized, bbox_resized, 1])  # 1 for nodule class\n",
        "        else:\n",
        "            for i in range(img_array.shape[0]):\n",
        "                img_resized = cv2.resize(img_array[i], target_size, interpolation=cv2.INTER_AREA)\n",
        "                data.append([img_file.split('/')[-1], i, img_resized, [0, 0, 0, 0], 0])  # 0 for non-nodule class\n",
        "    return pd.DataFrame(data, columns=[\"seriesuid\", \"sliceindex\", \"imagedata\", \"bbox\", \"class\"])\n",
        "\n",
        "# Combine nodules and non-nodules\n",
        "df_slices = load_images_with_nodules(file_list, df_node)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nq0tb-oVidd",
        "outputId": "96f65b26-0cde-4594-97e7-cdaa1fed1907"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images with nodules: 240\n",
            "Number of images without nodules: 10234\n"
          ]
        }
      ],
      "source": [
        "# Count the number of images with nodules and without nodules\n",
        "num_images_with_nodules = df_slices[df_slices['class'] == 1].shape[0]\n",
        "num_images_without_nodules = df_slices[df_slices['class'] == 0].shape[0]\n",
        "\n",
        "print(f\"Number of images with nodules: {num_images_with_nodules}\")\n",
        "print(f\"Number of images without nodules: {num_images_without_nodules}\")\n",
        "\n",
        "# Verify the counts\n",
        "assert num_images_with_nodules + num_images_without_nodules == len(df_slices), \"Counts do not match total number of images\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Data Preprocessing and Augmentation\n",
        "\n",
        "The `preprocess_and_save_images` function saves the preprocessed images as .pt files to improve velocity in the running. The `NoduleDataset` class loads these preprocessed images and their corresponding bounding boxes.\n",
        "\n",
        "Data augmentation techniques such as random horizontal and vertical flips are applied to the images. The preprocessed images are then split into training and validation sets.\n",
        "We also modify the RetinaNet model to handle our specific number of classes and add batch normalization to the convolutional layers.\n"
      ],
      "metadata": {
        "id": "hm0DmBY8mJGh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GNYEm3APBq6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "from torchvision.ops.boxes import box_iou\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "import torch.multiprocessing as mp\n",
        "\n",
        "# Set start method for multiprocessing to 'spawn'\n",
        "mp.set_start_method('spawn', force=True)\n",
        "\n",
        "# Function to preprocess and save images\n",
        "def preprocess_and_save_images(df, save_dir):\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        img = row['imagedata']\n",
        "        filename = f\"{row['seriesuid']}_{row['sliceindex']}.pt\"\n",
        "        filepath = os.path.join(save_dir, filename)\n",
        "        torch.save({'image': torch.tensor(img).float().unsqueeze(0), 'bbox': row['bbox'], 'class': row['class']}, filepath)\n",
        "\n",
        "# Preprocess and save images\n",
        "save_dir = '/content/preprocessed_images'\n",
        "preprocess_and_save_images(df_slices, save_dir)\n",
        "\n",
        "class NoduleDataset(Dataset):\n",
        "    def __init__(self, file_list, transform=None):\n",
        "        self.file_list = file_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = torch.load(self.file_list[idx])\n",
        "        image = data['image']\n",
        "        if data['class'] == 1:\n",
        "            bbox = torch.tensor(data['bbox']).float().unsqueeze(0)  # [num_boxes, 4]\n",
        "            target = {'boxes': bbox, 'labels': torch.tensor([1], dtype=torch.int64)}  # 1 for nodules\n",
        "        else:\n",
        "            target = {'boxes': torch.empty((0, 4)), 'labels': torch.tensor([0], dtype=torch.int64)}  # No bounding boxes for non-nodules\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, target\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "# Get list of preprocessed files\n",
        "file_list = glob(os.path.join(save_dir, '*.pt'))\n",
        "\n",
        "# Split the file list into training and validation sets\n",
        "train_files, val_files = train_test_split(file_list, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define data augmentation\n",
        "transform = T.Compose([\n",
        "    T.RandomHorizontalFlip(0.5),\n",
        "    T.RandomVerticalFlip(0.5),\n",
        "])\n",
        "\n",
        "# Create DataLoader with increased batch size\n",
        "batch_size = 32  # Increased batch size (tried with 16, 32 ang 64)\n",
        "train_dataset = NoduleDataset(train_files, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn, num_workers=2)\n",
        "\n",
        "val_dataset = NoduleDataset(val_files)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn, num_workers=2)\n",
        "\n",
        "# Load model\n",
        "num_classes = 2\n",
        "model = torchvision.models.detection.retinanet_resnet50_fpn_v2(weights=torchvision.models.detection.RetinaNet_ResNet50_FPN_V2_Weights.DEFAULT)\n",
        "model.head.classification_head.cls_logits = nn.Conv2d(model.head.classification_head.cls_logits.in_channels, model.head.classification_head.num_anchors * num_classes, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "model.head.regression_head.bbox_reg = nn.Conv2d(model.head.regression_head.bbox_reg.in_channels, model.head.classification_head.num_anchors * 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "model.head.classification_head.num_classes = num_classes\n",
        "\n",
        "# Add Batch Normalization\n",
        "for name, module in model.head.named_children():\n",
        "    if isinstance(module, nn.Conv2d):\n",
        "        setattr(model.head, name, nn.Sequential(\n",
        "            module,\n",
        "            nn.BatchNorm2d(module.out_channels)\n",
        "        ))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Model Training Setup\n",
        "\n",
        "Set up the training process for the model.\n",
        "The model is moved to the GPU if available.\n",
        "We use the AdamW optimizer with a tuned learning rate and a Cosine Annealing learning rate scheduler. Mixed precision training is enabled to speed up training and reduce memory usage.\n",
        "\n",
        "The `train_one_epoch` function handles the training of the model for one epoch. It includes the forward pass, loss computation, backpropagation, and optimization steps.\n",
        "\n",
        "We also ensure the loss is not NaN by checking and setting a large value if it is (which happened sometimes due to not performing well).\n"
      ],
      "metadata": {
        "id": "1GFVUrP2nJzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "torch.cuda.empty_cache()\n",
        "model.to(device)\n",
        "\n",
        "# Setup optimizer with AdamW\n",
        "learning_rate = 1e-4  # Tuned learning rate\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "\n",
        "# Using mixed precision training\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "# Training and evaluation loop\n",
        "train_losses = []\n",
        "val_acc_scores = []\n",
        "val_mse_scores = []\n",
        "best_model_wts = None\n",
        "best_val_acc = 0.0\n",
        "epochs = 30  # Increased number of epochs for better training\n",
        "\n",
        "def train_one_epoch(model, data_loader, optimizer, device, scaler):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for images, targets in tqdm(data_loader):\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(losses).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        epoch_loss += losses.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(data_loader)\n",
        "    avg_loss_tensor = torch.tensor(avg_loss, device=device)  # Convert to tensor for NaN check\n",
        "    if torch.isnan(avg_loss_tensor):\n",
        "        avg_loss = float('inf')  # Handle NaN values by setting a large value\n",
        "    print(f\"Epoch loss: {avg_loss}\")\n",
        "    return avg_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "Xj3JsL_-3gUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Model Evaluation and Visualization\n",
        "\n",
        "We evaluate the model and visualize the predictions. The `evaluate` function calculates the accuracy and mean squared error (MSE) of the model on the validation set. It also displays a confusion matrix to show the classification performance of the model in distinguishing between nodules and non-nodules.\n",
        "\n",
        "The `visualize_predictions` function plots the predicted and true bounding boxes on the images. This helps us visually inspect the performance of the model.\n",
        "\n",
        "We train the model for multiple epochs (30 defined before), evaluate its performance on the validation set, and save the best model weights based on the validation accuracy. Finally, we plot the training loss and validation metrics over the epochs.\n"
      ],
      "metadata": {
        "id": "KRkUq3jln8b4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    all_boxes_true = []\n",
        "    all_boxes_pred = []\n",
        "    mse_loss = nn.MSELoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in tqdm(data_loader):\n",
        "            images = list(image.to(device) for image in images)\n",
        "            outputs = model(images)\n",
        "\n",
        "            for target in targets:\n",
        "                if 'labels' in target:\n",
        "                    y_true.extend(target['labels'].cpu().numpy())\n",
        "                if 'boxes' in target:\n",
        "                    all_boxes_true.extend(target['boxes'].cpu().numpy())\n",
        "\n",
        "            for output in outputs:\n",
        "                if 'labels' in output:\n",
        "                    pred_labels = output['labels'].cpu().numpy()\n",
        "                    y_pred.extend(pred_labels)\n",
        "                if 'boxes' in output:\n",
        "                    pred_boxes = output['boxes'].cpu().numpy()\n",
        "                    all_boxes_pred.extend(pred_boxes)\n",
        "\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    if len(y_true) == 0 or len(y_pred) == 0:\n",
        "        print(\"No predictions or labels to compare.\")\n",
        "        return 0, float('inf')  # Handle cases where there are no predictions\n",
        "\n",
        "    min_len = min(len(y_true), len(y_pred))\n",
        "    y_true = y_true[:min_len]\n",
        "    y_pred = y_pred[:min_len]\n",
        "\n",
        "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
        "    mse = mse_loss(torch.tensor(all_boxes_pred[:min_len]), torch.tensor(all_boxes_true[:min_len])).item()\n",
        "\n",
        "    print(f'y_true length: {len(y_true)}')\n",
        "    print(f'y_pred length: {len(y_pred)}')\n",
        "    print(f'y_true: {y_true}')\n",
        "    print(f'y_pred: {y_pred}')\n",
        "\n",
        "    # Confusion Matrix\n",
        "    if len(y_true) > 0 and len(y_pred) > 0:\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Non-nodule', 'Nodule'])\n",
        "        disp.plot(cmap=plt.cm.Blues)\n",
        "        plt.show()\n",
        "\n",
        "    return accuracy, mse\n",
        "\n",
        "def visualize_predictions(model, data_loader, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, targets in data_loader:\n",
        "            images = list(image.to(device) for image in images)\n",
        "            outputs = model(images)\n",
        "            for img, target, output in zip(images, targets, outputs):\n",
        "                img = img.cpu().numpy().transpose(1, 2, 0)\n",
        "                img = (img - img.min()) / (img.max() - img.min())\n",
        "                plt.imshow(img, cmap='gray')\n",
        "\n",
        "                true_boxes = target['boxes'].cpu().numpy()\n",
        "                for box in true_boxes:\n",
        "                    plt.gca().add_patch(plt.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1], fill=False, edgecolor='green', linewidth=2))\n",
        "\n",
        "                pred_boxes = output['boxes'].cpu().numpy()\n",
        "                for box in pred_boxes:\n",
        "                    plt.gca().add_patch(plt.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1], fill=False, edgecolor='red', linewidth=2))\n",
        "\n",
        "                plt.show()\n",
        "                break  # Show only one batch for brevity\n",
        "\n",
        "\n",
        "# Training and evaluation\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, device, scaler)\n",
        "    val_loss = evaluate(model, val_loader, device)\n",
        "    visualize_predictions(model, val_loader, device)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    val_acc, val_mse = evaluate(model, val_loader, device)\n",
        "    val_acc_scores.append(val_acc)\n",
        "    val_mse_scores.append(val_mse)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Training Loss: {train_loss}, Validation Accuracy: {val_acc}, Validation MSE: {val_mse}')\n",
        "\n",
        "    # Adjust learning rate\n",
        "    scheduler.step(train_loss)\n",
        "\n",
        "    # Save best model weights\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model_wts = model.state_dict()\n",
        "        torch.save(best_model_wts, 'best_model_weights.pth')\n",
        "        print('Saved best model weights')\n",
        "\n",
        "print(f\"Total number of samples in training dataset: {len(train_dataset)}\")\n",
        "\n",
        "# Plotting loss and metrics\n",
        "epochs_range = range(1, epochs + 1)\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range\n"
      ],
      "metadata": {
        "id": "21fvtgiI3KJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXZpxpsAJMWd"
      },
      "source": [
        "##TESTING PART"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oy7N82n4JOnL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import SimpleITK as sitk\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from google.colab import drive\n",
        "\n",
        "# Assuming NoduleDataset and other necessary functions are already defined\n",
        "\n",
        "class NoduleDataset(Dataset):\n",
        "    def __init__(self, file_list):\n",
        "        self.file_list = file_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.file_list[idx]\n",
        "        image = self.load_image(file_path)\n",
        "        bbox = self.get_bbox(file_path)\n",
        "        return image, bbox\n",
        "\n",
        "    def load_image(self, file_path):\n",
        "        itk_img = sitk.ReadImage(file_path)\n",
        "        img_array = sitk.GetArrayFromImage(itk_img)\n",
        "        img_array = img_array.astype(np.float32)\n",
        "        img_array = (img_array - np.min(img_array)) / (np.max(img_array) - np.min(img_array))\n",
        "        img_array = np.expand_dims(img_array, axis=0)  # Add channel dimension\n",
        "        return torch.tensor(img_array)\n",
        "\n",
        "    def get_bbox(self, file_path):\n",
        "        # Implement the logic to get bounding box for the given image\n",
        "        # Here we return a dummy bbox, replace this with your actual bbox logic\n",
        "        bbox = [0, 0, 50, 50]  # Example bbox, replace with actual\n",
        "        return {'boxes': torch.tensor([bbox], dtype=torch.float32), 'labels': torch.tensor([1], dtype=torch.int64)}\n",
        "\n",
        "# Define subset9 path\n",
        "subset9_path = \"/content/drive/Shareddrives/IA DL_project/ML IA/LUNA16/subsets/subset9\"\n",
        "file_list = glob(subset9_path + \"/*.mhd\")\n",
        "\n",
        "# Create subset9 dataset\n",
        "subset9_dataset = NoduleDataset(file_list)\n",
        "subset9_loader = DataLoader(subset9_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "# Function to perform testing\n",
        "def test_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    all_boxes = []\n",
        "    all_scores = []\n",
        "    all_labels = []\n",
        "    all_true_boxes = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in data_loader:\n",
        "            images = list(image.to(device) for image in images)\n",
        "            outputs = model(images)\n",
        "\n",
        "            for i, output in enumerate(outputs):\n",
        "                pred_boxes = output['boxes'].cpu().numpy()\n",
        "                pred_scores = output['scores'].cpu().numpy()\n",
        "                pred_labels = output['labels'].cpu().numpy()\n",
        "                true_boxes = targets[i]['boxes'].cpu().numpy()\n",
        "\n",
        "                all_boxes.append(pred_boxes)\n",
        "                all_scores.append(pred_scores)\n",
        "                all_labels.append(pred_labels)\n",
        "                all_true_boxes.append(true_boxes)\n",
        "\n",
        "                # Visualize the detection\n",
        "                image = images[i].cpu().numpy().squeeze(0)\n",
        "                plt.imshow(image, cmap='gray')\n",
        "                ax = plt.gca()\n",
        "                for box in pred_boxes:\n",
        "                    rect = plt.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], fill=False, color='r')\n",
        "                    ax.add_patch(rect)\n",
        "                for box in true_boxes:\n",
        "                    rect = plt.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], fill=False, color='g')\n",
        "                    ax.add_patch(rect)\n",
        "                plt.show()\n",
        "\n",
        "    return all_boxes, all_scores, all_labels, all_true_boxes\n",
        "\n",
        "# Function to plot FROC\n",
        "def plot_froc(true_boxes, pred_boxes, pred_scores):\n",
        "    # Flatten the list of boxes and scores\n",
        "    true_boxes = [box for sublist in true_boxes for box in sublist]\n",
        "    pred_boxes = [box for sublist in pred_boxes for box in sublist]\n",
        "    pred_scores = [score for sublist in pred_scores for score in sublist]\n",
        "\n",
        "    # Calculate true positive rates and false positive rates\n",
        "    fpr, tpr, _ = roc_curve([1] * len(true_boxes) + [0] * len(pred_boxes), pred_scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Plot the FROC curve\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('FROC Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "# Load the best model\n",
        "best_model = torch.load('best_model_weights.pth')\n",
        "best_model.to(device)\n",
        "\n",
        "# Perform testing\n",
        "all_boxes, all_scores, all_labels, all_true_boxes = test_model(best_model, subset9_loader, device)\n",
        "\n",
        "# Plot FROC\n",
        "plot_froc(all_true_boxes, all_boxes, all_scores)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}